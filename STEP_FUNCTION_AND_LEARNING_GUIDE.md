# STEP_FUNCTION_AND_LEARNING_GUIDE.md

# Step Function, Reward System & Model Learning

## 1. THE STEP FUNCTION (What Happens Each Step)

### Structure
```python
def step(self, action):
    # Input: action (0-7) from agent
    # Output: observation, reward, terminated, truncated, info
    
    # A. Execute action
    # B. Calculate reward
    # C. Check termination
    # D. Render (optional)
    # E. Return new state
```

### Timeline of a Single Step

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 0: Agent is at (0,14) with 0 passengers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Agent observes state:
    obs = [âˆ’1, âˆ’1, âˆ’1, âˆ’1, ...]  (15 features)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent chooses ACTION 2 (Pick up passengers)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Environment EXECUTES action:
    - Add 4-8 random passengers
    - Agent now has ~6 passengers
    - Agent still at (0,14)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REWARD CALCULATION                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ Base reward:           +0.6  (progress)         â”‚
â”‚ Pickup reward:         +1.2Ã—6 = +7.2           â”‚
â”‚ Total reward:          +7.8                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Check termination:
    - Is episode finished? (reached Posta or crashed)
    - Has 350 steps elapsed?
    - triggered = False, truncated = False
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: New observation returned to agent       â”‚
â”‚ obs = [âˆ’1, âˆ’1, âˆ’0.76, âˆ’1, ...]  (state changed)â”‚
â”‚ reward = +7.8                                   â”‚
â”‚ terminated = False                              â”‚
â”‚ truncated = False                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. THE REWARD SYSTEM (How Agent Gets Feedback)

### Reward Table (Complete)

#### Progress & Delivery
| Event | Reward | Why |
|-------|--------|-----|
| Each step forward | +0.6 | Encourage efficiency (reach destination faster) |
| Each passenger dropped | +1.2 Ã— passengers | Proportional to passengers delivered |
| Reach Posta (destination) | +100 | Success bonus |
| Perfect legal trip (â‰¤33 pax at Posta) | +200 | Bonus for obeying law |

#### Safety & Compliance  
| Event | Reward | Why |
|-------|--------|-----|
| Stop voluntarily at police/red light | +6 | Reward safe behavior |
| Ignore police/red light (actionâ‰ 1) | -45 | Penalize recklessness |
| Reject bribe | +30 | Reward honesty |
| Accept bribe | +15 | Allow some flexibility |

#### Penalties (Safety & Law)
| Event | Reward | Why |
|-------|--------|-----|
| Light overload (34-40 pax at police) | -40 | Discourage overloading |
| Heavy overload (>40 pax at police) | -200 + TERMINATED | Severe penalty |
| Accident (overloaded + reckless) | -400 + TERMINATED | Worst case |
| Truncation (350 steps, no success) | -50 | Discourage inefficiency |

### Real Example: 3-Step Episode

```
STEP 0 (Initial):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Position: (0,14), Passengers: 0
Observation: [âˆ’1, âˆ’1, âˆ’1, âˆ’1, ...]
Action: 0 (Move forward)
Reward: +0.6 (progress)
Position: (1,14), Passengers: 0
Cumulative: +0.6

STEP 1:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Position: (1,14), Passengers: 0
At high-demand stop? YES (4,14) is coming
Action: 2 (Pick up passengers)
Passengers: +6 added
Reward: +0.6 (progress) + 1.2Ã—6 (pickup) = +7.8
Cumulative: +0.6 + 7.8 = +8.4

STEP 2:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Position: (1,14), Passengers: 6
Approaching police checkpoint at (6,14)
Action: 0 (Move forward)
Position: (2,14), Passengers: 6
Reward: +0.6 (progress)
Cumulative: +8.4 + 0.6 = +9.0
```

---

## 3. HOW THE MODEL LEARNS

### The Learning Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPISODE 1                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agent takes random actions                          â”‚
â”‚ Accumulates experience: (obs, action, reward)       â”‚
â”‚ Episode ends with total reward = 50                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEARNING (Update Policy/Value Function)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Algorithm analyzes: "What actions led to reward?"   â”‚
â”‚ Updates weights: Increase prob of good actions      â”‚
â”‚ Decrease prob of bad actions                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPISODE 2                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agent uses updated policy (slightly smarter)        â”‚
â”‚ Accumulates experience                              â”‚
â”‚ Episode ends with total reward = 75 (improved!)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
         (Repeat 300,000 times...)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPISODE 150,000                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agent has learned good strategy!                    â”‚
â”‚ Accumulates experience                              â”‚
â”‚ Episode ends with total reward = 250 (expert!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Each Algorithm Learns

#### **DQN (Value-Based)**
Learns: "What is the VALUE of each state-action pair?"

```
Q(state, action) = Expected future reward if I take this action

Example learning:
State: (pos=5, passengers=10, police_ahead=True)

Q(state, action=0) = Low value  (moving forward is risky)
Q(state, action=1) = High value (stopping is safe)
Q(state, action=6) = Medium value (accepting bribe is risky)

Agent chooses action with highest Q value
```

#### **PPO (Policy Gradient)**
Learns: "What is the PROBABILITY of each action being good?"

```
Policy Ï€(action|state) = Probability of taking this action

Example learning:
State: (pos=5, passengers=10, police_ahead=True)

Ï€(action=0) = 0.2  (20% chance move forward)
Ï€(action=1) = 0.7  (70% chance stop)
Ï€(action=6) = 0.1  (10% chance accept bribe)

Agent samples from distribution (usually picks action=1, sometimes explores)
```

#### **A2C (Actor-Critic)**
Learns: BOTH value (Critic) + probability (Actor)

```
Critic: "Is this state good?" (Value function)
Actor: "What should I do here?" (Policy)

Example:
State: (pos=5, passengers=10, police_ahead=True)

Critic says: V(state) = 50 (moderate value state)
Actor says: Ï€(action=1) = 0.8 (stop is best)

Agent: "This state is okay, and stopping is the right move"
```

#### **REINFORCE (Pure Policy Gradient)**
Learns: "Maximize total episode reward by adjusting policy"

```
At end of episode: Reward = 150
Backward pass: "Which actions contributed to this reward?"

Actions that led to high reward:
- Stop at police â†’ weight UP
- Pick up passengers â†’ weight UP
- Move forward at yellow light â†’ weight DOWN

Policy updated to repeat good actions
```

---

## 4. TRAINING CONVERGENCE

### What "Learning" Looks Like

```
Episode Rewards Over Time:

Reward
  â”‚
  â”‚                                      â•±â•±â•±â•±â•±
300â”‚                              â•±â•±â•±â•±â•±â•±
  â”‚                        â•±â•±â•±â•±â•±â•±
250â”‚                  â•±â•±â•±â•±â•±
  â”‚            â•±â•±â•±â•±â•±
200â”‚      â•±â•±â•±â•±â•±
  â”‚  â•±â•±â•±â•±â•±
150â”‚â•±â•±â•±â•±â•± (Convergence)
  â”‚
100â”‚
  â”‚
 50â”‚ (Random exploration)
  â”‚
  0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0      50,000   100,000   150,000   300,000
              Training Steps

Agent gradually improves!
```

### Why Does This Happen?

**Early training (random actions)**:
- Agent tries everything
- Accumulates data on what works
- Rewards are mixed

**Middle training (learning)**:
- Patterns emerge
- Agent starts taking better actions
- Rewards increase

**Late training (convergence)**:
- Agent has learned optimal policy
- Repeats successful strategies
- Rewards plateau at high level

---

## 5. CONCRETE EXAMPLE: Agent Learning

### Episode 1 (Random)
```
Step 0: At (0,14), pax=0
  Action: 1 (Stop)  â† Random choice
  Reward: +0.6
  Observation: [âˆ’1, âˆ’1, âˆ’1, ...]

Step 1: At (0,14), pax=0
  Action: 4 (Hard accelerate)  â† Random choice
  Reward: +0.6
  Observation: [âˆ’1, âˆ’1, âˆ’1, ...]

Step 2: At (1,14), pax=0
  Action: 7 (Reject bribe)  â† Random, but no bribe offered
  Reward: +0 (no effect)
  Observation: [âˆ’1, âˆ’1, âˆ’1, ...]

... (continuing randomly)

Total Episode Reward: 45 (mediocre)
```

### Episode 100 (Slight Learning)
```
Step 0: At (0,14), pax=0
  Action: 0 (Move forward)  â† Agent learned this is good
  Reward: +0.6
  Observation: [âˆ’0.86, âˆ’1, âˆ’1, ...]

Step 1: At (1,14), pax=0
  Action: 0 (Move forward)  â† Continue
  Reward: +0.6
  Observation: [âˆ’0.72, âˆ’1, âˆ’1, ...]

Step 2: At (2,14), pax=0
  Action: 0 (Move forward)
  Reward: +0.6
  Observation: [âˆ’0.58, âˆ’1, âˆ’1, ...]

Step 3: At (3,14), pax=0
  Action: 0 (Move forward) â†’ Wait, traffic light ahead!
  Reward: âˆ’45 (violated red light!)
  OBSERVATION: Agent learns "moving on red light = BAD"
  Observation: [âˆ’0.44, âˆ’1, âˆ’1, ...]

Step 4: At (3,14), pax=0
  Action: 1 (Stop)
  Reward: +6 (correct! Stopped at red light)
  Observation: [âˆ’0.44, âˆ’1, âˆ’1, ...]

Step 5: At (4,14), pax=0  â† HIGH-DEMAND STOP
  Action: 2 (Pick up)
  Passengers: +6
  Reward: +0.6 + 1.2Ã—6 = +7.8
  Observation: [âˆ’0.44, âˆ’1, âˆ’0.76, ...]

... (continues with better strategy)

Total Episode Reward: 120 (improving!)
```

### Episode 1000 (Converged)
```
Step 0: At (0,14), pax=0
  Action: 0 (Move forward)
  Reward: +0.6
  (continues moving...)

Step 4: At (4,14) â† HIGH-DEMAND STOP
  Action: 2 (Pick up passengers)
  Passengers: +6
  Reward: +7.8

Step 6: At (6,14) â† POLICE CHECKPOINT
  Must_stop_next: True (sees police ahead)
  Action: 1 (Stop voluntarily)
  Reward: +6 (safe behavior!)

Step 7: At (6,14) â† AT POLICE
  Passengers: 6 (legal, â‰¤33)
  Action: 0 (Continue)
  Reward: +0.6 (no fine)

... (continues optimally)

Step 28: At (14,0) â† DESTINATION (Posta)
  Passengers: 22 (successfully delivered)
  Action: 0 (Move forward/reached)
  Reward: +100 (destination) + 0 (not â‰¤33, so no +200)
  TERMINATED: True

Total Episode Reward: 280 (excellent!)
```

---

## 6. KEY INSIGHTS

### What the Model Actually Learns

Through repeated episodes, the agent discovers:

1. **Route knowledge**
   - "High-demand stops at (4,14), (8,14), (14,8), (14,3)"
   - "Police checkpoints at (6,14), (11,14), (14,10)"
   - "Traffic lights at (3,14), (10,14), (14,12), (14,5)"

2. **Risk-reward trade-off**
   - "Picking up passengers = +reward, but overload risk"
   - "Speeding = risky when overloaded"
   - "Bribes = risky, usually reject"

3. **Optimal strategy**
   - "Operate at 34-38 passengers (profitable + survivable)"
   - "Always stop at police (reward outweighs risk)"
   - "Reject ~75% of bribes"
   - "Move forward on green, stop on red"

4. **Efficiency**
   - "Deliver passengers at appropriate stops"
   - "Don't waste time idling"
   - "Reach destination within 350 steps"

### Success Criteria

Agent has learned well when:
- âœ“ Mean reward > 200
- âœ“ Legal compliance > 50% (â‰¤33 passengers at end)
- âœ“ Crash rate < 10%
- âœ“ Reaches destination reliably

---

## 7. COMPARISON: 4 ALGORITHMS

| Algorithm | What It Learns | Speed | Stability |
|-----------|-----------------|-------|-----------|
| **DQN** | State-action values | Slow-Medium | Medium |
| **PPO** | Action probabilities | Fast | High |
| **A2C** | Values + probabilities | Medium | High |
| **REINFORCE** | Episode rewards | Slow | Low |

---

## Summary

```
FLOW:
Agent Observes â†’ Chooses Action â†’ Gets Reward â†’ Learns â†’ Better Next Time

REWARD:
Profit-driven (passengers, delivery)
- Safety-constrained (traffic laws, capacity)
- Corruption-tested (bribes)

LEARNING:
Happens automatically through 300,000 timesteps
Each algorithm explores differently
Convergence = agent has discovered optimal strategy

RESULT:
Agent learns to balance safety + profit âœ“
```

You now have a complete picture of step â†’ reward â†’ learning! ğŸ¯
