{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cfc98d",
   "metadata": {},
   "source": [
    "# PPO Training on Google Colab\n",
    "\n",
    "This notebook contains a complete, self-contained PPO (Proximal Policy Optimization) training pipeline for the Daladala environment.\n",
    "\n",
    "**What's Included:**\n",
    "- Full environment definition (5 actions, 14 observations)\n",
    "- 12 PPO hyperparameter configurations for systematic tuning\n",
    "- Training loop with 300,000 timesteps per configuration\n",
    "- Automatic best model tracking and evaluation\n",
    "- Results saved directly to Google Drive\n",
    "\n",
    "**Expected Runtime:** ~3-4 hours on Colab CPU for all 12 configurations\n",
    "**Output:** Best model + detailed results JSON saved to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfa5a2",
   "metadata": {},
   "source": [
    "## Section 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 gymnasium torch pandas numpy opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c489f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnNoModelImprovement\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcf6c7",
   "metadata": {},
   "source": [
    "## Section 2: Mount Google Drive (for saving models and results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09575b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/models/ppo', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/results', exist_ok=True)\n",
    "print(\"✓ Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25d3cd",
   "metadata": {},
   "source": [
    "## Section 3: Define the DaladalaEnv Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaladalaEnv(gym.Env):\n",
    "    \"\"\"Daladala (mini-bus) optimization environment with 5 actions and 14 observations.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 12}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(-1, 1, shape=(14,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(5)  # Move, Stop, Pickup, Dropoff, SpeedUp\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Fixed route Ubungo → Posta (right then up)\n",
    "        self.route = [(x, 14) for x in range(15)] + [(14, y) for y in range(13, -1, -1)]\n",
    "        self.high_demand_stops = [(4,14), (8,14), (14,8), (14,3)]\n",
    "        self.police_checkpoints = [(6,14), (11,14), (14,10)]\n",
    "        self.traffic_lights = [(3,14), (10,14), (14,12), (14,5)]\n",
    "\n",
    "        self.max_steps = 350\n",
    "        self.physical_max = 50\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.passengers = 0\n",
    "        self.money = 0.0\n",
    "        self.pos_idx = 0\n",
    "        self.speed = 0\n",
    "        self.fined = False\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.pos_idx >= len(self.route):\n",
    "            x, y = 14, 0\n",
    "        else:\n",
    "            x, y = self.route[self.pos_idx]\n",
    "\n",
    "        # Get next cell info\n",
    "        next_idx = min(self.pos_idx + 1, len(self.route)-1)\n",
    "        next_x, next_y = self.route[next_idx]\n",
    "\n",
    "        # Traffic light logic\n",
    "        light_red = 0\n",
    "        if (x,y) in self.traffic_lights:\n",
    "            light_red = 1 if (self.step_count // 40) % 2 == 0 else 0\n",
    "\n",
    "        # Police & must_stop detection\n",
    "        police_checkpoint_ahead = 1 if (next_x, next_y) in self.police_checkpoints else 0\n",
    "        must_stop_next = 1 if police_checkpoint_ahead or (light_red and (x,y) in self.traffic_lights) else 0\n",
    "\n",
    "        # Calculate distances\n",
    "        dist_to_next_light = float('inf')\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.traffic_lights:\n",
    "                dist_to_next_light = i - self.pos_idx\n",
    "                break\n",
    "        dist_to_next_light = min(dist_to_next_light / 5.0, 1.0)\n",
    "\n",
    "        dist_to_next_police = float('inf')\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.police_checkpoints:\n",
    "                dist_to_next_police = i - self.pos_idx\n",
    "                break\n",
    "        dist_to_next_police = min(dist_to_next_police / 5.0, 1.0)\n",
    "\n",
    "        # Passengers waiting at high-demand stops\n",
    "        if not hasattr(self, 'passengers_waiting_state'):\n",
    "            self.passengers_waiting_state = {}\n",
    "        \n",
    "        if (x, y) in self.high_demand_stops:\n",
    "            if (x, y) not in self.passengers_waiting_state:\n",
    "                self.passengers_waiting_state[(x, y)] = np.random.randint(0, 11)\n",
    "        else:\n",
    "            self.passengers_waiting_state = {}\n",
    "        \n",
    "        passengers_waiting = self.passengers_waiting_state.get((x, y), 0) / 10.0\n",
    "\n",
    "        # Normalize all observations to [-1, 1]\n",
    "        obs = np.array([\n",
    "            x / 14.0 * 2 - 1,\n",
    "            y / 14.0 * 2 - 1,\n",
    "            self.passengers / 50.0 * 2 - 1,\n",
    "            self.money / 150000.0 * 2 - 1,\n",
    "            self.speed / 3.0,\n",
    "            dist_to_next_light * 2 - 1,\n",
    "            dist_to_next_police * 2 - 1,\n",
    "            light_red * 2 - 1,\n",
    "            must_stop_next * 2 - 1,\n",
    "            1 if (x,y) in self.high_demand_stops else -1,\n",
    "            passengers_waiting * 2 - 1,\n",
    "            1 if self.passengers > 40 else -1,\n",
    "            1 if self.fined else -1,\n",
    "            self.step_count / self.max_steps * 2 - 1\n",
    "        ], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        terminated = truncated = False\n",
    "\n",
    "        x, y = self.route[self.pos_idx]\n",
    "        must_stop_now = self._must_stop_here()\n",
    "\n",
    "        # Capture pre-action state\n",
    "        prev_passengers = self.passengers\n",
    "        prev_money = self.money\n",
    "        prev_pos_idx = self.pos_idx\n",
    "\n",
    "        # Execute action (pure state transitions, no conditions)\n",
    "        if action == 0:  # Move Forward\n",
    "            if self.pos_idx < len(self.route) - 1:\n",
    "                self.pos_idx += 1\n",
    "                self.speed = min(self.speed + 1, 3)\n",
    "            else:\n",
    "                terminated = True\n",
    "\n",
    "        elif action == 1:  # Stop\n",
    "            self.speed = 0\n",
    "\n",
    "        elif action == 2:  # Pick up passengers\n",
    "            if (x, y) in self.high_demand_stops and self.passengers < self.physical_max:\n",
    "                base_add = np.random.randint(4, 9)\n",
    "                waiting_count = int(self.passengers_waiting_state.get((x, y), 0))\n",
    "                add = min(base_add + waiting_count // 2, self.physical_max - self.passengers)\n",
    "                self.passengers = min(self.passengers + add, self.physical_max)\n",
    "                if (x, y) in self.passengers_waiting_state:\n",
    "                    self.passengers_waiting_state[(x, y)] = max(0, waiting_count - add)\n",
    "\n",
    "        elif action == 3:  # Drop off passengers\n",
    "            if (x, y) in self.high_demand_stops and self.passengers > 0:\n",
    "                drop = min(self.passengers, np.random.randint(6, 16))\n",
    "                self.passengers -= drop\n",
    "                self.money += drop * 1000\n",
    "\n",
    "        elif action == 4:  # Speed Up\n",
    "            if self.passengers <= 40:\n",
    "                self.speed = min(self.speed + 1, 3)\n",
    "\n",
    "        # Measure outcomes (pure state deltas)\n",
    "        pos_progress = self.pos_idx - prev_pos_idx\n",
    "        passengers_added = self.passengers - prev_passengers\n",
    "        passengers_dropped = prev_passengers - self.passengers\n",
    "        money_earned = self.money - prev_money\n",
    "\n",
    "        # Calculate reward from outcomes only\n",
    "        reward = 0.0\n",
    "\n",
    "        if pos_progress > 0:\n",
    "            reward += 5\n",
    "        \n",
    "        if passengers_added > 0:\n",
    "            reward += passengers_added * 1.0\n",
    "        \n",
    "        if passengers_dropped > 0:\n",
    "            reward += passengers_dropped * 1.2\n",
    "        \n",
    "        if money_earned > 0:\n",
    "            reward += money_earned / 20000.0\n",
    "\n",
    "        if terminated:\n",
    "            reward += 100\n",
    "            if self.passengers <= 33:\n",
    "                reward += 200\n",
    "\n",
    "        # Safety consequences\n",
    "        if action == 0 and must_stop_now:\n",
    "            reward -= 45\n",
    "        \n",
    "        if action == 1 and not must_stop_now:\n",
    "            reward -= 2\n",
    "        \n",
    "        if action == 1 and must_stop_now:\n",
    "            reward += 6\n",
    "        \n",
    "        if action == 4 and self.passengers > 40:\n",
    "            reward -= 400\n",
    "            terminated = True\n",
    "        \n",
    "        if (x, y) in self.police_checkpoints:\n",
    "            if self.passengers > 40:\n",
    "                reward -= 200\n",
    "                terminated = True\n",
    "                self.fined = True\n",
    "            elif self.passengers > 33:\n",
    "                reward -= 40\n",
    "                self.fined = True\n",
    "\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "    def _must_stop_here(self):\n",
    "        if self.pos_idx >= len(self.route)-1:\n",
    "            return False\n",
    "        nx, ny = self.route[self.pos_idx + 1]\n",
    "        light_red = 0\n",
    "        cx, cy = self.route[self.pos_idx]\n",
    "        if (cx,cy) in self.traffic_lights:\n",
    "            light_red = 1 if (self.step_count // 40) % 2 == 0 else 0\n",
    "        return (nx,ny) in self.police_checkpoints or light_red == 1\n",
    "\n",
    "    def render(self):\n",
    "        pass  # Rendering disabled for Colab\n",
    "\n",
    "print(\"✓ DaladalaEnv class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772eee1",
   "metadata": {},
   "source": [
    "## Section 4: Define PPO Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 PPO hyperparameter configurations for systematic tuning\n",
    "ppo_configs = [\n",
    "    {\"name\": \"LR_1e4_n_steps_512\", \"learning_rate\": 1e-4, \"n_steps\": 512, \"batch_size\": 64, \"gamma\": 0.99, \"clip_range\": 0.2, \"ent_coef\": 0.0},\n",
    "    {\"name\": \"LR_1e4_n_steps_1024\", \"learning_rate\": 1e-4, \"n_steps\": 1024, \"batch_size\": 64, \"gamma\": 0.99, \"clip_range\": 0.2, \"ent_coef\": 0.0},\n",
    "    {\"name\": \"LR_3e4_n_steps_512\", \"learning_rate\": 3e-4, \"n_steps\": 512, \"batch_size\": 128, \"gamma\": 0.995, \"clip_range\": 0.2, \"ent_coef\": 0.005},\n",
    "    {\"name\": \"LR_3e4_n_steps_1024\", \"learning_rate\": 3e-4, \"n_steps\": 1024, \"batch_size\": 128, \"gamma\": 0.995, \"clip_range\": 0.2, \"ent_coef\": 0.005},\n",
    "    {\"name\": \"LR_5e4_n_steps_512\", \"learning_rate\": 5e-4, \"n_steps\": 512, \"batch_size\": 128, \"gamma\": 0.99, \"clip_range\": 0.2, \"ent_coef\": 0.01},\n",
    "    {\"name\": \"LR_5e4_n_steps_1024\", \"learning_rate\": 5e-4, \"n_steps\": 1024, \"batch_size\": 64, \"gamma\": 0.995, \"clip_range\": 0.2, \"ent_coef\": 0.01},\n",
    "    {\"name\": \"LR_7e4_n_steps_512\", \"learning_rate\": 7e-4, \"n_steps\": 512, \"batch_size\": 64, \"gamma\": 0.99, \"clip_range\": 0.25, \"ent_coef\": 0.0},\n",
    "    {\"name\": \"LR_7e4_n_steps_1024\", \"learning_rate\": 7e-4, \"n_steps\": 1024, \"batch_size\": 128, \"gamma\": 0.995, \"clip_range\": 0.25, \"ent_coef\": 0.005},\n",
    "    {\"name\": \"LR_1e3_n_steps_512\", \"learning_rate\": 1e-3, \"n_steps\": 512, \"batch_size\": 64, \"gamma\": 0.99, \"clip_range\": 0.2, \"ent_coef\": 0.01},\n",
    "    {\"name\": \"LR_1e3_n_steps_1024\", \"learning_rate\": 1e-3, \"n_steps\": 1024, \"batch_size\": 128, \"gamma\": 0.995, \"clip_range\": 0.2, \"ent_coef\": 0.01},\n",
    "    {\"name\": \"LR_1e3_n_steps_2048\", \"learning_rate\": 1e-3, \"n_steps\": 2048, \"batch_size\": 128, \"gamma\": 0.99, \"clip_range\": 0.15, \"ent_coef\": 0.0},\n",
    "    {\"name\": \"LR_5e4_n_steps_2048\", \"learning_rate\": 5e-4, \"n_steps\": 2048, \"batch_size\": 64, \"gamma\": 0.995, \"clip_range\": 0.25, \"ent_coef\": 0.005},\n",
    "]\n",
    "\n",
    "print(f\"✓ {len(ppo_configs)} PPO configurations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267ccf6",
   "metadata": {},
   "source": [
    "## Section 5: Train PPO with All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c62a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_reward = -float('inf')\n",
    "best_config = None\n",
    "best_model = None\n",
    "\n",
    "total_configs = len(ppo_configs)\n",
    "\n",
    "for idx, config in enumerate(ppo_configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Configuration {idx}/{total_configs}: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = DaladalaEnv()\n",
    "    \n",
    "    # Initialize PPO model with configuration\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        n_steps=config['n_steps'],\n",
    "        batch_size=config['batch_size'],\n",
    "        gamma=config['gamma'],\n",
    "        clip_range=config['clip_range'],\n",
    "        ent_coef=config['ent_coef'],\n",
    "        verbose=0,\n",
    "        device='cpu'\n",
    "    )\n",
    "    \n",
    "    # Train for 300,000 timesteps\n",
    "    print(f\"Training for 300,000 timesteps...\")\n",
    "    model.learn(total_timesteps=300000)\n",
    "    \n",
    "    # Evaluate model on 50 episodes\n",
    "    print(f\"Evaluating on 50 episodes...\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50, deterministic=True)\n",
    "    \n",
    "    results[config['name']] = {\n",
    "        'config': config,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Mean Reward: {mean_reward:.2f} (±{std_reward:.2f})\")\n",
    "    \n",
    "    # Track best model\n",
    "    if mean_reward > best_reward:\n",
    "        best_reward = mean_reward\n",
    "        best_config = config['name']\n",
    "        best_model = model\n",
    "        print(f\"★ New best model!\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Configuration: {best_config}\")\n",
    "print(f\"Best Mean Reward: {best_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b1960",
   "metadata": {},
   "source": [
    "## Section 6: Save Best Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17949484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_path = '/content/drive/MyDrive/daladala_results/models/ppo/best_ppo'\n",
    "best_model.save(best_model_path)\n",
    "print(f\"✓ Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Save results as JSON\n",
    "results_json_path = '/content/drive/MyDrive/daladala_results/results/ppo_results.json'\n",
    "results_summary = {}\n",
    "for config_name, config_results in results.items():\n",
    "    results_summary[config_name] = {\n",
    "        'mean_reward': float(config_results['mean_reward']),\n",
    "        'std_reward': float(config_results['std_reward']),\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': config_results['config']['learning_rate'],\n",
    "            'n_steps': config_results['config']['n_steps'],\n",
    "            'batch_size': config_results['config']['batch_size'],\n",
    "            'gamma': config_results['config']['gamma'],\n",
    "            'clip_range': config_results['config']['clip_range'],\n",
    "            'ent_coef': config_results['config']['ent_coef']\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"✓ Results saved to: {results_json_path}\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY - All 12 PPO Configurations\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Config': name,\n",
    "        'Mean Reward': f\"{results[name]['mean_reward']:.2f}\",\n",
    "        'Std Reward': f\"{results[name]['std_reward']:.2f}\",\n",
    "        'LR': results[name]['config']['learning_rate'],\n",
    "        'n_steps': results[name]['config']['n_steps'],\n",
    "        'gamma': results[name]['config']['gamma']\n",
    "    }\n",
    "    for name in results.keys()\n",
    "])\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8be062",
   "metadata": {},
   "source": [
    "## Section 7: Test Best Model on Sample Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model on 5 sample episodes\n",
    "print(f\"\\nTesting best model ({best_config}) on 5 sample episodes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = DaladalaEnv()\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = best_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1}: Reward = {total_reward:.2f} (Steps: {steps})\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample Episodes Mean Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Sample Episodes Std Reward:  {np.std(episode_rewards):.2f}\")\n",
    "print(\"\\n✓ PPO training and evaluation complete!\")\n",
    "print(f\"✓ Models and results saved to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
