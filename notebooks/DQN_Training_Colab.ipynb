{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d666529e",
   "metadata": {},
   "source": [
    "# DQN Training on Google Colab\n",
    "\n",
    "This notebook contains a complete, self-contained DQN (Deep Q-Network) training pipeline for the Daladala environment.\n",
    "\n",
    "**What's Included:**\n",
    "- Full environment definition (5 actions, 14 observations)\n",
    "- 12 DQN hyperparameter configurations for systematic tuning\n",
    "- Training loop with 300,000 timesteps per configuration\n",
    "- Automatic best model tracking and evaluation\n",
    "- Results saved directly to Google Drive\n",
    "\n",
    "**Expected Runtime:** ~3-4 hours on Colab CPU for all 12 configurations\n",
    "**Output:** Best model + detailed results JSON saved to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a44eb",
   "metadata": {},
   "source": [
    "## Section 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794de51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 gymnasium torch pandas numpy opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eaa210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnNoModelImprovement\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad215143",
   "metadata": {},
   "source": [
    "## Section 2: Mount Google Drive (for saving models and results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/models/dqn', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/results', exist_ok=True)\n",
    "print(\"✓ Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023fbc",
   "metadata": {},
   "source": [
    "## Section 3: Define the DaladalaEnv Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad04ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaladalaEnv(gym.Env):\n",
    "    \"\"\"Daladala (mini-bus) optimization environment with automatic movement and randomized hazards.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 12}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(-1, 1, shape=(14,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(5)  # 0:Move, 1:Pickup, 2:Dropoff, 3:Stop, 4:SpeedUp\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Fixed route Ubungo → Posta (right then up)\n",
    "        self.route = [(x, 14) for x in range(15)] + [(14, y) for y in range(13, -1, -1)]\n",
    "        self.high_demand_stops = [(4,14), (8,14), (14,8), (14,3)]\n",
    "        \n",
    "        # These will be randomized each reset\n",
    "        self.police_checkpoints = []\n",
    "        self.traffic_lights = []\n",
    "\n",
    "        self.max_steps = 350\n",
    "        self.physical_max = 50\n",
    "        self.light_cycle = 0  # Track light cycle deterministically\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.passengers = 0\n",
    "        self.money = 0.0\n",
    "        self.pos_idx = 0\n",
    "        self.speed = 0\n",
    "        self.fined = False\n",
    "        self.light_cycle = 0\n",
    "        \n",
    "        # Randomize hazard positions each episode\n",
    "        available = [pos for pos in self.route if pos not in self.high_demand_stops]\n",
    "        if len(available) >= 7:  # 3 police + 4 traffic lights\n",
    "            sampled = np.random.choice(len(available), 7, replace=False)\n",
    "            self.police_checkpoints = [available[i] for i in sampled[:3]]\n",
    "            self.traffic_lights = [available[i] for i in sampled[3:7]]\n",
    "        \n",
    "        # Initialize deterministic passenger counts per stop (seeded by position)\n",
    "        self.passengers_at_stop = {}\n",
    "        for stop in self.high_demand_stops:\n",
    "            # Deterministic: same stop always has same initial count\n",
    "            seed_val = hash(stop) % 11  # 0-10 passengers\n",
    "            self.passengers_at_stop[stop] = seed_val\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Generate observation based on current location and environment state.\n",
    "        Observations are DETERMINISTIC per location to match training/visualization.\n",
    "        \"\"\"\n",
    "        if self.pos_idx >= len(self.route):\n",
    "            x, y = 14, 0\n",
    "        else:\n",
    "            x, y = self.route[self.pos_idx]\n",
    "\n",
    "        # === CURRENT LOCATION HAZARDS ===\n",
    "        # Traffic light: RED on even cycles, GREEN on odd cycles\n",
    "        light_is_red = 1 if (x, y) in self.traffic_lights and (self.light_cycle % 2 == 0) else 0\n",
    "        \n",
    "        # Police checkpoint detection\n",
    "        police_here = 1 if (x, y) in self.police_checkpoints else 0\n",
    "        \n",
    "        # Must stop at THIS location?\n",
    "        must_stop_now = 1 if (light_is_red or police_here) else 0\n",
    "        \n",
    "        # === NEXT LOCATION HAZARDS ===\n",
    "        next_idx = min(self.pos_idx + 1, len(self.route) - 1)\n",
    "        next_x, next_y = self.route[next_idx]\n",
    "        \n",
    "        # Check what's ahead\n",
    "        next_light_is_red = 1 if (next_x, next_y) in self.traffic_lights and (self.light_cycle % 2 == 0) else 0\n",
    "        police_ahead = 1 if (next_x, next_y) in self.police_checkpoints else 0\n",
    "        must_stop_next = 1 if (next_light_is_red or police_ahead) else 0\n",
    "        \n",
    "        # === PASSENGER STATE ===\n",
    "        # At high-demand stop: passengers waiting (deterministic)\n",
    "        at_stop = 1 if (x, y) in self.high_demand_stops else 0\n",
    "        passengers_waiting = self.passengers_at_stop.get((x, y), 0) if at_stop else 0\n",
    "        \n",
    "        # === DISTANCE AHEAD (for lookahead) ===\n",
    "        # Distance to next traffic light (in next 5 cells)\n",
    "        dist_to_light = 5\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.traffic_lights:\n",
    "                dist_to_light = i - self.pos_idx\n",
    "                break\n",
    "        \n",
    "        # Distance to next police (in next 5 cells)\n",
    "        dist_to_police = 5\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.police_checkpoints:\n",
    "                dist_to_police = i - self.pos_idx\n",
    "                break\n",
    "        \n",
    "        # === BUILD OBSERVATION VECTOR (all normalized to [-1, 1]) ===\n",
    "        obs = np.array([\n",
    "            x / 14.0 * 2 - 1,                      # [0] position_x\n",
    "            y / 14.0 * 2 - 1,                      # [1] position_y\n",
    "            self.passengers / self.physical_max * 2 - 1,  # [2] current_passengers\n",
    "            self.money / 150000.0 * 2 - 1,        # [3] money_earned\n",
    "            self.speed / 3.0 * 2 - 1,              # [4] current_speed\n",
    "            light_is_red * 2 - 1,                  # [5] light_is_red_HERE (critical)\n",
    "            police_here * 2 - 1,                   # [6] police_checkpoint_HERE\n",
    "            must_stop_now * 2 - 1,                 # [7] must_stop_now_HERE (critical)\n",
    "            at_stop * 2 - 1,                       # [8] at_high_demand_stop\n",
    "            passengers_waiting / 10.0 * 2 - 1,    # [9] passengers_waiting_at_stop\n",
    "            must_stop_next * 2 - 1,                # [10] must_stop_next_location\n",
    "            dist_to_light / 5.0 * 2 - 1,          # [11] distance_to_traffic_light\n",
    "            dist_to_police / 5.0 * 2 - 1,         # [12] distance_to_police\n",
    "            self.step_count / self.max_steps * 2 - 1,  # [13] episode_progress\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Action: 0=Move, 1=Pickup, 2=Dropoff, 3=Stop, 4=SpeedUp\n",
    "        Movement is ALWAYS automatic. Actions are overlaid on movement.\n",
    "        Rewards guide agent toward optimal actions based on current state.\n",
    "        \"\"\"\n",
    "        self.step_count += 1\n",
    "        self.light_cycle += 1  # Update traffic light cycle\n",
    "        \n",
    "        terminated = truncated = False\n",
    "        x, y = self.route[self.pos_idx]\n",
    "        \n",
    "        # === PHASE 1: AUTOMATIC MOVEMENT (always happens) ===\n",
    "        if self.pos_idx < len(self.route) - 1:\n",
    "            self.pos_idx += 1\n",
    "        else:\n",
    "            terminated = True\n",
    "        \n",
    "        # === PHASE 2: EXECUTE ACTION ===\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Observe the CURRENT location (before action)\n",
    "        light_is_red = 1 if (x, y) in self.traffic_lights and (self.light_cycle % 2 == 0) else 0\n",
    "        police_here = 1 if (x, y) in self.police_checkpoints else 0\n",
    "        must_stop_here = 1 if (light_is_red or police_here) else 0\n",
    "        at_stop = 1 if (x, y) in self.high_demand_stops else 0\n",
    "        \n",
    "        # === INTELLIGENT REWARD SYSTEM ===\n",
    "        # We know the \"right\" action for each state, so rewards guide strongly\n",
    "        \n",
    "        if action == 0:  # MOVE action (advance to next cell)\n",
    "            # Movement already happened automatically\n",
    "            # This action is mostly for consistency; reward small progress bonus\n",
    "            reward += 2\n",
    "            \n",
    "            # PENALTY: Moved through hazard without stopping\n",
    "            if must_stop_here:\n",
    "                reward -= 40  # Heavy penalty: ran through red light or police checkpoint\n",
    "        \n",
    "        elif action == 1:  # PICKUP action\n",
    "            if at_stop and self.passengers < self.physical_max:\n",
    "                # GOOD: Picking up at a stop\n",
    "                base_add = max(3, self.passengers_at_stop.get((x, y), 0))\n",
    "                add = min(base_add, self.physical_max - self.passengers)\n",
    "                self.passengers += add\n",
    "                reward += 15  # High reward for correct action\n",
    "                \n",
    "                # Deduct waiting passengers\n",
    "                if (x, y) in self.passengers_at_stop:\n",
    "                    self.passengers_at_stop[(x, y)] = max(0, self.passengers_at_stop[(x, y)] - add)\n",
    "            else:\n",
    "                # BAD: Picked up when not at stop\n",
    "                reward -= 5\n",
    "            \n",
    "            # PENALTY: Picking up at hazard zone\n",
    "            if must_stop_here:\n",
    "                reward -= 10\n",
    "        \n",
    "        elif action == 2:  # DROPOFF action\n",
    "            if at_stop and self.passengers > 0:\n",
    "                # GOOD: Dropping off at a stop\n",
    "                drop = min(self.passengers, max(3, self.passengers // 2 + 1))\n",
    "                self.passengers -= drop\n",
    "                self.money += drop * 1000\n",
    "                reward += 12  # Good reward for revenue\n",
    "            else:\n",
    "                # BAD: Dropped off when not at stop\n",
    "                reward -= 8\n",
    "            \n",
    "            # PENALTY: Dropping off at hazard zone\n",
    "            if must_stop_here:\n",
    "                reward -= 10\n",
    "        \n",
    "        elif action == 3:  # STOP action (slows down / waits)\n",
    "            self.speed = max(0, self.speed - 1)\n",
    "            \n",
    "            # GOOD: Stopped at hazard location\n",
    "            if must_stop_here:\n",
    "                reward += 25  # Strong reward: correct safety action\n",
    "            else:\n",
    "                # BAD: Unnecessary stop\n",
    "                reward -= 3\n",
    "        \n",
    "        elif action == 4:  # SPEEDUP action\n",
    "            # GOOD: Speeding up in safe zones\n",
    "            if not must_stop_here and self.passengers <= 40:\n",
    "                self.speed = min(self.speed + 1, 3)\n",
    "                reward += 3\n",
    "            else:\n",
    "                # BAD: Speeding in danger or when overloaded\n",
    "                if must_stop_here:\n",
    "                    reward -= 15  # Dangerous\n",
    "                if self.passengers > 40:\n",
    "                    reward -= 30  # Could crash\n",
    "                    terminated = True  # Crash!\n",
    "        \n",
    "        # === PHASE 3: SAFETY VIOLATIONS ===\n",
    "        # Check destination after automatic movement\n",
    "        new_x, new_y = self.route[self.pos_idx] if self.pos_idx < len(self.route) else (14, 0)\n",
    "        \n",
    "        # Police checkpoint consequences\n",
    "        if (new_x, new_y) in self.police_checkpoints:\n",
    "            if self.passengers > 40:\n",
    "                reward -= 50  # Severe: overloaded at police\n",
    "                self.fined = True\n",
    "                terminated = True\n",
    "            elif self.passengers > 33:\n",
    "                reward -= 20  # Violation: illegal capacity\n",
    "                self.fined = True\n",
    "        \n",
    "        # === PHASE 4: PROGRESS & COMPLETION ===\n",
    "        # Base movement reward (small, to encourage progress)\n",
    "        reward += 1\n",
    "        \n",
    "        # Destination completion bonus\n",
    "        if terminated:\n",
    "            reward += 100  # Large bonus for reaching destination\n",
    "            if self.passengers <= 33 and not self.fined:\n",
    "                reward += 50  # Bonus for legal completion\n",
    "        \n",
    "        # === PHASE 5: STATE UPDATES ===\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass  # Rendering disabled for Colab\n",
    "\n",
    "print(\"✓ DaladalaEnv class defined with new intelligent design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615a6af",
   "metadata": {},
   "source": [
    "## Section 4: Define DQN Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 DQN hyperparameter configurations for systematic tuning\n",
    "dqn_configs = [\n",
    "    {\"name\": \"LR_1e4_buf_10k_eps_025\", \"learning_rate\": 1e-4, \"buffer_size\": 10000, \"exploration_fraction\": 0.25, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.99},\n",
    "    {\"name\": \"LR_1e4_buf_50k_eps_05\", \"learning_rate\": 1e-4, \"buffer_size\": 50000, \"exploration_fraction\": 0.5, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.99},\n",
    "    {\"name\": \"LR_3e4_buf_10k_eps_025\", \"learning_rate\": 3e-4, \"buffer_size\": 10000, \"exploration_fraction\": 0.25, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.995},\n",
    "    {\"name\": \"LR_3e4_buf_50k_eps_05\", \"learning_rate\": 3e-4, \"buffer_size\": 50000, \"exploration_fraction\": 0.5, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.995},\n",
    "    {\"name\": \"LR_5e4_buf_10k_eps_025\", \"learning_rate\": 5e-4, \"buffer_size\": 10000, \"exploration_fraction\": 0.25, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.1, \"gamma\": 0.99},\n",
    "    {\"name\": \"LR_5e4_buf_50k_eps_05\", \"learning_rate\": 5e-4, \"buffer_size\": 50000, \"exploration_fraction\": 0.5, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.1, \"gamma\": 0.995},\n",
    "    {\"name\": \"LR_7e4_buf_10k_eps_025\", \"learning_rate\": 7e-4, \"buffer_size\": 10000, \"exploration_fraction\": 0.25, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.02, \"gamma\": 0.99},\n",
    "    {\"name\": \"LR_7e4_buf_50k_eps_05\", \"learning_rate\": 7e-4, \"buffer_size\": 50000, \"exploration_fraction\": 0.5, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.02, \"gamma\": 0.995},\n",
    "    {\"name\": \"LR_1e3_buf_10k_eps_025\", \"learning_rate\": 1e-3, \"buffer_size\": 10000, \"exploration_fraction\": 0.25, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.99},\n",
    "    {\"name\": \"LR_1e3_buf_50k_eps_05\", \"learning_rate\": 1e-3, \"buffer_size\": 50000, \"exploration_fraction\": 0.5, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.995},\n",
    "    {\"name\": \"LR_1e3_buf_100k_eps_1\", \"learning_rate\": 1e-3, \"buffer_size\": 100000, \"exploration_fraction\": 1.0, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.05, \"gamma\": 0.99},\n",
    "    {\"name\": \"LR_5e4_buf_100k_eps_1\", \"learning_rate\": 5e-4, \"buffer_size\": 100000, \"exploration_fraction\": 1.0, \"exploration_initial_eps\": 1.0, \"exploration_final_eps\": 0.1, \"gamma\": 0.995},\n",
    "]\n",
    "\n",
    "print(f\"✓ {len(dqn_configs)} DQN configurations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696dfc5",
   "metadata": {},
   "source": [
    "## Section 5: Train DQN with All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901349a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_reward = -float('inf')\n",
    "best_config = None\n",
    "best_model = None\n",
    "\n",
    "total_configs = len(dqn_configs)\n",
    "\n",
    "for idx, config in enumerate(dqn_configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Configuration {idx}/{total_configs}: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = DaladalaEnv()\n",
    "    \n",
    "    # Initialize DQN model with configuration\n",
    "    model = DQN(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        exploration_fraction=config['exploration_fraction'],\n",
    "        exploration_initial_eps=config['exploration_initial_eps'],\n",
    "        exploration_final_eps=config['exploration_final_eps'],\n",
    "        gamma=config['gamma'],\n",
    "        verbose=0,\n",
    "        device='cpu'\n",
    "    )\n",
    "    \n",
    "    # Train for 300,000 timesteps\n",
    "    print(f\"Training for 300,000 timesteps...\")\n",
    "    model.learn(total_timesteps=300000)\n",
    "    \n",
    "    # Evaluate model on 50 episodes\n",
    "    print(f\"Evaluating on 50 episodes...\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50, deterministic=True)\n",
    "    \n",
    "    results[config['name']] = {\n",
    "        'config': config,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Mean Reward: {mean_reward:.2f} (±{std_reward:.2f})\")\n",
    "    \n",
    "    # Track best model\n",
    "    if mean_reward > best_reward:\n",
    "        best_reward = mean_reward\n",
    "        best_config = config['name']\n",
    "        best_model = model\n",
    "        print(f\"★ New best model!\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Configuration: {best_config}\")\n",
    "print(f\"Best Mean Reward: {best_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd571c2",
   "metadata": {},
   "source": [
    "## Section 6: Save Best Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_path = '/content/drive/MyDrive/daladala_results/models/dqn/best_dqn'\n",
    "best_model.save(best_model_path)\n",
    "print(f\"✓ Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Save results as JSON\n",
    "results_json_path = '/content/drive/MyDrive/daladala_results/results/dqn_results.json'\n",
    "results_summary = {}\n",
    "for config_name, config_results in results.items():\n",
    "    results_summary[config_name] = {\n",
    "        'mean_reward': float(config_results['mean_reward']),\n",
    "        'std_reward': float(config_results['std_reward']),\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': config_results['config']['learning_rate'],\n",
    "            'buffer_size': config_results['config']['buffer_size'],\n",
    "            'exploration_fraction': config_results['config']['exploration_fraction'],\n",
    "            'exploration_initial_eps': config_results['config']['exploration_initial_eps'],\n",
    "            'exploration_final_eps': config_results['config']['exploration_final_eps'],\n",
    "            'gamma': config_results['config']['gamma']\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"✓ Results saved to: {results_json_path}\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY - All 12 DQN Configurations\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Config': name,\n",
    "        'Mean Reward': f\"{results[name]['mean_reward']:.2f}\",\n",
    "        'Std Reward': f\"{results[name]['std_reward']:.2f}\",\n",
    "        'LR': results[name]['config']['learning_rate'],\n",
    "        'Buffer': results[name]['config']['buffer_size'],\n",
    "        'Gamma': results[name]['config']['gamma']\n",
    "    }\n",
    "    for name in results.keys()\n",
    "])\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346b06b",
   "metadata": {},
   "source": [
    "## Section 7: Test Best Model on Sample Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bb8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model on 5 sample episodes\n",
    "print(f\"\\nTesting best model ({best_config}) on 5 sample episodes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = DaladalaEnv()\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = best_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1}: Reward = {total_reward:.2f} (Steps: {steps})\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample Episodes Mean Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Sample Episodes Std Reward:  {np.std(episode_rewards):.2f}\")\n",
    "print(\"\\n✓ DQN training and evaluation complete!\")\n",
    "print(f\"✓ Models and results saved to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
