{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39b7bbd2",
      "metadata": {
        "id": "39b7bbd2"
      },
      "source": [
        "# REINFORCE Training\n",
        "\n",
        "This notebook contains a complete, self-contained REINFORCE (Policy Gradient) training pipeline for the Daladala environment.\n",
        "\n",
        "**What's Included:**\n",
        "- Full environment definition (5 actions, 14 observations)\n",
        "- 12 REINFORCE hyperparameter configurations for systematic tuning\n",
        "- Training loop with 300,000 timesteps per configuration\n",
        "\n",
        "**Output:** Best model + detailed results JSON saved to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9365ab01",
      "metadata": {
        "id": "9365ab01"
      },
      "source": [
        "## Section 1: Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097a2adf",
      "metadata": {
        "id": "097a2adf"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium torch pandas numpy opencv-python --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf5877e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daf5877e",
        "outputId": "4a1d191b-12a7-436b-9cb6-d6ed28e4f837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"✓ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  CUDA Version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9163cd6a",
      "metadata": {
        "id": "9163cd6a"
      },
      "source": [
        "## Section 2: Mount Google Drive (for saving models and results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712f3890",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712f3890",
        "outputId": "2843fe53-4a97-4a70-ed06-ae5cf2fd3624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted successfully\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs('/content/drive/MyDrive/daladala_results/models/reinforce', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/daladala_results/results', exist_ok=True)\n",
        "print(\"✓ Google Drive mounted successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07be0212",
      "metadata": {
        "id": "07be0212"
      },
      "source": [
        "## Section 3: Define the DaladalaEnv Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a05b039",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a05b039",
        "outputId": "82d2350f-11d7-4c75-87d5-90d09288ddf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ DaladalaEnv class defined successfully\n"
          ]
        }
      ],
      "source": [
        "class DaladalaEnv(gym.Env):\n",
        "    \"\"\"Daladala (mini-bus) optimization environment with 5 actions and 14 observations.\"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 12}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.observation_space = spaces.Box(-1, 1, shape=(14,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(5)  # 0:Move, 1:Pickup, 2:Dropoff, 3:Stop, 4:SpeedUp\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Fixed route Ubungo → Posta (right then up)\n",
        "        self.route = [(x, 14) for x in range(15)] + [(14, y) for y in range(13, -1, -1)]\n",
        "        self.high_demand_stops = [(4,14), (8,14), (14,8), (14,3)]\n",
        "\n",
        "        # These will be randomized each reset\n",
        "        self.police_checkpoints = []\n",
        "        self.traffic_lights = []\n",
        "        self.traffic_light_states = {}  # Stores state (Red=1, Green=0) for each light\n",
        "\n",
        "        # Pools for randomization\n",
        "        self.available_positions = [pos for pos in self.route if pos not in self.high_demand_stops]\n",
        "\n",
        "        self.max_steps = 350\n",
        "        self.physical_max = 50\n",
        "        self.light_cycle = 0  # Track light cycle deterministically\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.step_count = 0\n",
        "        self.passengers = 0\n",
        "        self.money = 0.0\n",
        "        self.pos_idx = 0\n",
        "        self.speed = 0\n",
        "        self.fined = False\n",
        "        self.light_cycle = 0\n",
        "\n",
        "        # Randomize hazard positions each episode\n",
        "        available = [pos for pos in self.route if pos not in self.high_demand_stops]\n",
        "        if len(available) >= 7:  # 3 police + 4 traffic lights\n",
        "            sampled = np.random.choice(len(available), 7, replace=False)\n",
        "            self.police_checkpoints = [available[i] for i in sampled[:3]]\n",
        "            self.traffic_lights = [available[i] for i in sampled[3:7]]\n",
        "\n",
        "            # Assign random constant state (Red=1, Green=0) for each light this episode\n",
        "            # This ensures the light stays the same throughout the episode\n",
        "            self.traffic_light_states = {pos: np.random.randint(0, 2) for pos in self.traffic_lights}\n",
        "\n",
        "        # Initialize deterministic passenger counts per stop (seeded by position)\n",
        "        self.passengers_at_stop = {}\n",
        "        for stop in self.high_demand_stops:\n",
        "            # Deterministic: same stop always has same initial count\n",
        "            seed_val = hash(stop) % 11  # 0-10 passengers\n",
        "            self.passengers_at_stop[stop] = seed_val\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"\n",
        "        Generate observation based on current location and environment state.\n",
        "        Observations are DETERMINISTIC per location to match training/visualization.\n",
        "        \"\"\"\n",
        "        if self.pos_idx >= len(self.route):\n",
        "            x, y = 14, 0\n",
        "        else:\n",
        "            x, y = self.route[self.pos_idx]\n",
        "\n",
        "        # === CURRENT LOCATION HAZARDS ===\n",
        "        # Traffic light: Constant state for this episode (Red=1, Green=0)\n",
        "        light_is_red = self.traffic_light_states.get((x, y), 0)\n",
        "\n",
        "        # Police checkpoint detection\n",
        "        police_here = 1 if (x, y) in self.police_checkpoints else 0\n",
        "\n",
        "        # Must stop at THIS location?\n",
        "        must_stop_now = 1 if (light_is_red or police_here) else 0\n",
        "\n",
        "        # === NEXT LOCATION HAZARDS ===\n",
        "        next_idx = min(self.pos_idx + 1, len(self.route) - 1)\n",
        "        next_x, next_y = self.route[next_idx]\n",
        "\n",
        "        # Check what's ahead\n",
        "        next_light_is_red = self.traffic_light_states.get((next_x, next_y), 0)\n",
        "        police_ahead = 1 if (next_x, next_y) in self.police_checkpoints else 0\n",
        "        must_stop_next = 1 if (next_light_is_red or police_ahead) else 0\n",
        "\n",
        "        # === PASSENGER STATE ===\n",
        "        # At high-demand stop: passengers waiting (deterministic)\n",
        "        at_stop = 1 if (x, y) in self.high_demand_stops else 0\n",
        "        passengers_waiting = self.passengers_at_stop.get((x, y), 0) if at_stop else 0\n",
        "\n",
        "        # === DISTANCE AHEAD (for lookahead) ===\n",
        "        # Distance to next traffic light (in next 5 cells)\n",
        "        dist_to_light = 5\n",
        "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
        "            if self.route[i] in self.traffic_lights:\n",
        "                dist_to_light = i - self.pos_idx\n",
        "                break\n",
        "\n",
        "        # Distance to next police (in next 5 cells)\n",
        "        dist_to_police = 5\n",
        "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
        "            if self.route[i] in self.police_checkpoints:\n",
        "                dist_to_police = i - self.pos_idx\n",
        "                break\n",
        "\n",
        "        # === BUILD OBSERVATION VECTOR (all normalized to [-1, 1]) ===\n",
        "        obs = np.array([\n",
        "            x / 14.0 * 2 - 1,                      # [0] position_x\n",
        "            y / 14.0 * 2 - 1,                      # [1] position_y\n",
        "            self.passengers / self.physical_max * 2 - 1,  # [2] current_passengers\n",
        "            self.money / 150000.0 * 2 - 1,        # [3] money_earned\n",
        "            self.speed / 3.0 * 2 - 1,              # [4] current_speed\n",
        "            light_is_red * 2 - 1,                  # [5] light_is_red_HERE (critical)\n",
        "            police_here * 2 - 1,                   # [6] police_checkpoint_HERE\n",
        "            must_stop_now * 2 - 1,                 # [7] must_stop_now_HERE (critical)\n",
        "            at_stop * 2 - 1,                       # [8] at_high_demand_stop\n",
        "            passengers_waiting / 10.0 * 2 - 1,    # [9] passengers_waiting_at_stop\n",
        "            must_stop_next * 2 - 1,                # [10] must_stop_next_location\n",
        "            dist_to_light / 5.0 * 2 - 1,          # [11] distance_to_traffic_light\n",
        "            dist_to_police / 5.0 * 2 - 1,         # [12] distance_to_police\n",
        "            self.step_count / self.max_steps * 2 - 1,  # [13] episode_progress\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Action: 0=Move, 1=Pickup, 2=Dropoff, 3=Stop, 4=SpeedUp\n",
        "        Movement is ALWAYS automatic. Actions are overlaid on movement.\n",
        "        Rewards guide agent toward optimal actions based on current state.\n",
        "        \"\"\"\n",
        "        self.step_count += 1\n",
        "        self.light_cycle += 1  # Update traffic light cycle\n",
        "\n",
        "        terminated = truncated = False\n",
        "        x, y = self.route[self.pos_idx]\n",
        "\n",
        "        # === PHASE 1: AUTOMATIC MOVEMENT (always happens) ===\n",
        "        if self.pos_idx < len(self.route) - 1:\n",
        "            self.pos_idx += 1\n",
        "        else:\n",
        "            terminated = True\n",
        "\n",
        "        # === PHASE 2: EXECUTE ACTION ===\n",
        "        reward = 0.0\n",
        "\n",
        "        # Observe the CURRENT location (before action)\n",
        "        light_is_red = self.traffic_light_states.get((x, y), 0)\n",
        "        police_here = 1 if (x, y) in self.police_checkpoints else 0\n",
        "        must_stop_here = 1 if (light_is_red or police_here) else 0\n",
        "        at_stop = 1 if (x, y) in self.high_demand_stops else 0\n",
        "\n",
        "        # === INTELLIGENT REWARD SYSTEM ===\n",
        "        # We know the \"right\" action for each state, so rewards guide strongly\n",
        "\n",
        "        if action == 0:  # MOVE action (advance to next cell)\n",
        "            # Movement already happened automatically\n",
        "            # This action is mostly for consistency; reward small progress bonus\n",
        "            reward += 2\n",
        "\n",
        "            # PENALTY: Moved through hazard without stopping\n",
        "            if must_stop_here:\n",
        "                reward -= 40  # Heavy penalty: ran through red light or police checkpoint\n",
        "\n",
        "        elif action == 1:  # PICKUP action\n",
        "            if at_stop and self.passengers < self.physical_max:\n",
        "                # GOOD: Picking up at a stop\n",
        "                base_add = max(3, self.passengers_at_stop.get((x, y), 0))\n",
        "                add = min(base_add, self.physical_max - self.passengers)\n",
        "                self.passengers += add\n",
        "                reward += 15  # High reward for correct action\n",
        "\n",
        "                # Deduct waiting passengers\n",
        "                if (x, y) in self.passengers_at_stop:\n",
        "                    self.passengers_at_stop[(x, y)] = max(0, self.passengers_at_stop[(x, y)] - add)\n",
        "            else:\n",
        "                # BAD: Picked up when not at stop\n",
        "                reward -= 5\n",
        "\n",
        "            # PENALTY: Picking up at hazard zone\n",
        "            if must_stop_here:\n",
        "                reward -= 10\n",
        "\n",
        "        elif action == 2:  # DROPOFF action\n",
        "            if at_stop and self.passengers > 0:\n",
        "                # GOOD: Dropping off at a stop\n",
        "                drop = min(self.passengers, max(3, self.passengers // 2 + 1))\n",
        "                self.passengers -= drop\n",
        "                self.money += drop * 1000\n",
        "                reward += 12  # Good reward for revenue\n",
        "            else:\n",
        "                # BAD: Dropped off when not at stop\n",
        "                reward -= 8\n",
        "\n",
        "            # PENALTY: Dropping off at hazard zone\n",
        "            if must_stop_here:\n",
        "                reward -= 10\n",
        "\n",
        "        elif action == 3:  # STOP action (slows down / waits)\n",
        "            self.speed = max(0, self.speed - 1)\n",
        "\n",
        "            # GOOD: Stopped at hazard location\n",
        "            if must_stop_here:\n",
        "                reward += 25  # Strong reward: correct safety action\n",
        "            else:\n",
        "                # BAD: Unnecessary stop\n",
        "                reward -= 3\n",
        "\n",
        "        elif action == 4:  # SPEEDUP action\n",
        "            # GOOD: Speeding up in safe zones\n",
        "            if not must_stop_here and self.passengers <= 40:\n",
        "                self.speed = min(self.speed + 1, 3)\n",
        "                reward += 3\n",
        "            else:\n",
        "                # BAD: Speeding in danger or when overloaded\n",
        "                if must_stop_here:\n",
        "                    reward -= 15  # Dangerous\n",
        "                if self.passengers > 40:\n",
        "                    reward -= 30  # Could crash\n",
        "                    terminated = True  # Crash!\n",
        "\n",
        "        # === PHASE 3: SAFETY VIOLATIONS ===\n",
        "        # Check destination after automatic movement\n",
        "        new_x, new_y = self.route[self.pos_idx] if self.pos_idx < len(self.route) else (14, 0)\n",
        "\n",
        "        # Police checkpoint consequences\n",
        "        if (new_x, new_y) in self.police_checkpoints:\n",
        "            if self.passengers > 40:\n",
        "                reward -= 50  # Severe: overloaded at police\n",
        "                self.fined = True\n",
        "                terminated = True\n",
        "            elif self.passengers > 33:\n",
        "                reward -= 20  # Violation: illegal capacity\n",
        "                self.fined = True\n",
        "\n",
        "        # === PHASE 4: PROGRESS & COMPLETION ===\n",
        "        # Base movement reward (small, to encourage progress)\n",
        "        reward += 1\n",
        "\n",
        "        # Destination completion bonus\n",
        "        if terminated:\n",
        "            reward += 100  # Large bonus for reaching destination\n",
        "            if self.passengers <= 33 and not self.fined:\n",
        "                reward += 50  # Bonus for legal completion\n",
        "\n",
        "        # === PHASE 5: STATE UPDATES ===\n",
        "        truncated = self.step_count >= self.max_steps\n",
        "\n",
        "        return self._get_obs(), reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self):\n",
        "        pass  # Rendering disabled for Colab\n",
        "\n",
        "print(\"✓ DaladalaEnv class defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ce5542",
      "metadata": {
        "id": "15ce5542"
      },
      "source": [
        "## Section 4: Define REINFORCE Policy Network and Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbee83c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbbee83c",
        "outputId": "bc7d3ef1-7f5b-41a1-f626-91f0c7095bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ REINFORCE Policy Network and Agent defined successfully\n"
          ]
        }
      ],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"Neural network policy for REINFORCE algorithm.\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return torch.softmax(self.net(state), dim=-1)\n",
        "\n",
        "    def get_action_and_log_prob(self, state):\n",
        "        \"\"\"Get action and log probability from policy.\"\"\"\n",
        "        probs = self.forward(state)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "\n",
        "class REINFORCEAgent:\n",
        "    \"\"\"REINFORCE (Policy Gradient) agent.\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, learning_rate, device='cpu'):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
        "        self.device = device\n",
        "        self.policy.to(self.device)\n",
        "\n",
        "    def train_episode(self, env):\n",
        "        \"\"\"Train for one complete episode.\"\"\"\n",
        "        obs, _ = env.reset()\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            action, log_prob = self.policy.get_action_and_log_prob(obs_tensor)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            done = terminated or truncated\n",
        "\n",
        "        # Calculate returns (discounted cumulative rewards)\n",
        "        returns = []\n",
        "        cumulative_return = 0\n",
        "        for reward in reversed(rewards):\n",
        "            cumulative_return = reward + 0.99 * cumulative_return\n",
        "            returns.insert(0, cumulative_return)\n",
        "\n",
        "        # Normalize returns\n",
        "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # Calculate policy loss\n",
        "        policy_loss = 0\n",
        "        for log_prob, return_val in zip(log_probs, returns):\n",
        "            policy_loss += -log_prob * return_val\n",
        "\n",
        "        # Update policy\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return sum(rewards)\n",
        "\n",
        "    def evaluate(self, env, n_episodes=50):\n",
        "        \"\"\"Evaluate agent performance.\"\"\"\n",
        "        rewards = []\n",
        "        for _ in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    probs = self.policy(obs_tensor)\n",
        "                    action = probs.argmax(dim=-1).item()\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                done = terminated or truncated\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "        return np.mean(rewards), np.std(rewards)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Save model to disk.\"\"\"\n",
        "        torch.save(self.policy.state_dict(), path + '_policy.pth')\n",
        "\n",
        "    def load(self, path):\n",
        "        \"\"\"Load model from disk.\"\"\"\n",
        "        self.policy.load_state_dict(torch.load(path + '_policy.pth', map_location=self.device))\n",
        "\n",
        "print(\"✓ REINFORCE Policy Network and Agent defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01559b15",
      "metadata": {
        "id": "01559b15"
      },
      "source": [
        "## Section 5: Define REINFORCE Hyperparameter Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5348eb62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5348eb62",
        "outputId": "494dfa64-b39f-43c1-e716-f2f5bea21164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 12 REINFORCE configurations defined\n"
          ]
        }
      ],
      "source": [
        "# 12 REINFORCE hyperparameter configurations for systematic tuning\n",
        "reinforce_configs = [\n",
        "    {\"name\": \"LR_1e3_hid_64\", \"learning_rate\": 1e-3, \"hidden_dim\": 64},\n",
        "    {\"name\": \"LR_1e3_hid_128\", \"learning_rate\": 1e-3, \"hidden_dim\": 128},\n",
        "    {\"name\": \"LR_3e3_hid_64\", \"learning_rate\": 3e-3, \"hidden_dim\": 64},\n",
        "    {\"name\": \"LR_3e3_hid_128\", \"learning_rate\": 3e-3, \"hidden_dim\": 128},\n",
        "    {\"name\": \"LR_5e3_hid_64\", \"learning_rate\": 5e-3, \"hidden_dim\": 64},\n",
        "    {\"name\": \"LR_5e3_hid_128\", \"learning_rate\": 5e-3, \"hidden_dim\": 128},\n",
        "    {\"name\": \"LR_1e2_hid_64\", \"learning_rate\": 1e-2, \"hidden_dim\": 64},\n",
        "    {\"name\": \"LR_1e2_hid_128\", \"learning_rate\": 1e-2, \"hidden_dim\": 128},\n",
        "    {\"name\": \"LR_1e2_hid_256\", \"learning_rate\": 1e-2, \"hidden_dim\": 256},\n",
        "    {\"name\": \"LR_5e3_hid_256\", \"learning_rate\": 5e-3, \"hidden_dim\": 256},\n",
        "    {\"name\": \"LR_3e3_hid_256\", \"learning_rate\": 3e-3, \"hidden_dim\": 256},\n",
        "    {\"name\": \"LR_1e3_hid_256\", \"learning_rate\": 1e-3, \"hidden_dim\": 256},\n",
        "]\n",
        "\n",
        "print(f\"✓ {len(reinforce_configs)} REINFORCE configurations defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d581dc93",
      "metadata": {
        "id": "d581dc93"
      },
      "source": [
        "## Section 6: Train REINFORCE with All Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbe93480",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbe93480",
        "outputId": "8973b9c4-2068-4d0a-d000-30d1bf5fb2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Training Configuration 1/12: LR_1e3_hid_64\n",
            "======================================================================\n",
            "Learning Rate: 0.001, Hidden Dim: 64\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:  117.00 | Last Reward:  117.00 | Steps: 350 | ETA:   3m 09s\n",
            "  Episode   50/858 | Recent Avg:   87.62 | Last Reward:   68.00 | Steps: 17,500 | ETA:   0m 26s\n",
            "  Episode  100/858 | Recent Avg:  106.98 | Last Reward:   74.00 | Steps: 35,000 | ETA:   0m 23s\n",
            "  Episode  150/858 | Recent Avg:  168.84 | Last Reward:  201.00 | Steps: 52,500 | ETA:   0m 21s\n",
            "  Episode  200/858 | Recent Avg:  257.44 | Last Reward:  282.00 | Steps: 70,000 | ETA:   0m 20s\n",
            "  Episode  250/858 | Recent Avg:  289.62 | Last Reward:  286.00 | Steps: 87,500 | ETA:   0m 19s\n",
            "  Episode  300/858 | Recent Avg:  297.54 | Last Reward:  317.00 | Steps: 105,000 | ETA:   0m 18s\n",
            "  Episode  350/858 | Recent Avg:  315.90 | Last Reward:  392.00 | Steps: 122,500 | ETA:   0m 18s\n",
            "  Episode  400/858 | Recent Avg:  338.26 | Last Reward:  369.00 | Steps: 140,000 | ETA:   0m 17s\n",
            "  Episode  450/858 | Recent Avg:  344.84 | Last Reward:  389.00 | Steps: 157,500 | ETA:   0m 15s\n",
            "  Episode  500/858 | Recent Avg:  370.12 | Last Reward:  410.00 | Steps: 175,000 | ETA:   0m 13s\n",
            "  Episode  550/858 | Recent Avg:  391.68 | Last Reward:  336.00 | Steps: 192,500 | ETA:   0m 11s\n",
            "  Episode  600/858 | Recent Avg:  394.58 | Last Reward:  396.00 | Steps: 210,000 | ETA:   0m 09s\n",
            "  Episode  650/858 | Recent Avg:  403.86 | Last Reward:  398.00 | Steps: 227,500 | ETA:   0m 07s\n",
            "  Episode  700/858 | Recent Avg:  397.40 | Last Reward:  412.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  414.52 | Last Reward:  396.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  415.58 | Last Reward:  402.00 | Steps: 280,000 | ETA:   0m 02s\n",
            "  Episode  850/858 | Recent Avg:  411.96 | Last Reward:  446.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 423.12 (±20.62)\n",
            "    Training Time: 0m 30s\n",
            "    ★ NEW BEST MODEL! ★\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 2/12: LR_1e3_hid_128\n",
            "======================================================================\n",
            "Learning Rate: 0.001, Hidden Dim: 128\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   51.00 | Last Reward:   51.00 | Steps: 350 | ETA:   0m 27s\n",
            "  Episode   50/858 | Recent Avg:   97.24 | Last Reward:  120.00 | Steps: 17,500 | ETA:   0m 24s\n",
            "  Episode  100/858 | Recent Avg:  223.88 | Last Reward:  314.00 | Steps: 35,000 | ETA:   0m 24s\n",
            "  Episode  150/858 | Recent Avg:  333.88 | Last Reward:  320.00 | Steps: 52,500 | ETA:   0m 25s\n",
            "  Episode  200/858 | Recent Avg:  349.66 | Last Reward:  332.00 | Steps: 70,000 | ETA:   0m 23s\n",
            "  Episode  250/858 | Recent Avg:  314.20 | Last Reward:  361.00 | Steps: 87,500 | ETA:   0m 21s\n",
            "  Episode  300/858 | Recent Avg:  336.64 | Last Reward:  337.00 | Steps: 105,000 | ETA:   0m 18s\n",
            "  Episode  350/858 | Recent Avg:  357.76 | Last Reward:  345.00 | Steps: 122,500 | ETA:   0m 16s\n",
            "  Episode  400/858 | Recent Avg:  354.54 | Last Reward:  349.00 | Steps: 140,000 | ETA:   0m 14s\n",
            "  Episode  450/858 | Recent Avg:  370.62 | Last Reward:  368.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:  371.20 | Last Reward:  323.00 | Steps: 175,000 | ETA:   0m 11s\n",
            "  Episode  550/858 | Recent Avg:  365.42 | Last Reward:  398.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:  382.98 | Last Reward:  399.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  375.58 | Last Reward:  348.00 | Steps: 227,500 | ETA:   0m 06s\n",
            "  Episode  700/858 | Recent Avg:  388.16 | Last Reward:  427.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  391.38 | Last Reward:  428.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  396.94 | Last Reward:  355.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:  386.46 | Last Reward:  443.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 402.62 (±22.73)\n",
            "    Training Time: 0m 27s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 3/12: LR_3e3_hid_64\n",
            "======================================================================\n",
            "Learning Rate: 0.003, Hidden Dim: 64\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   83.00 | Last Reward:   83.00 | Steps: 350 | ETA:   0m 27s\n",
            "  Episode   50/858 | Recent Avg:  116.36 | Last Reward:  154.00 | Steps: 17,500 | ETA:   0m 24s\n",
            "  Episode  100/858 | Recent Avg:  234.56 | Last Reward:  346.00 | Steps: 35,000 | ETA:   0m 26s\n",
            "  Episode  150/858 | Recent Avg:  345.56 | Last Reward:  348.00 | Steps: 52,500 | ETA:   0m 25s\n",
            "  Episode  200/858 | Recent Avg:  373.14 | Last Reward:  376.00 | Steps: 70,000 | ETA:   0m 22s\n",
            "  Episode  250/858 | Recent Avg:  371.90 | Last Reward:  354.00 | Steps: 87,500 | ETA:   0m 20s\n",
            "  Episode  300/858 | Recent Avg:  365.68 | Last Reward:  392.00 | Steps: 105,000 | ETA:   0m 18s\n",
            "  Episode  350/858 | Recent Avg:  374.18 | Last Reward:  354.00 | Steps: 122,500 | ETA:   0m 16s\n",
            "  Episode  400/858 | Recent Avg:  379.08 | Last Reward:  398.00 | Steps: 140,000 | ETA:   0m 14s\n",
            "  Episode  450/858 | Recent Avg:  375.96 | Last Reward:  398.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:  375.98 | Last Reward:  376.00 | Steps: 175,000 | ETA:   0m 11s\n",
            "  Episode  550/858 | Recent Avg:  377.76 | Last Reward:  354.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:  369.40 | Last Reward:  376.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  376.42 | Last Reward:  332.00 | Steps: 227,500 | ETA:   0m 06s\n",
            "  Episode  700/858 | Recent Avg:  379.08 | Last Reward:  354.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  372.48 | Last Reward:  376.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  377.76 | Last Reward:  398.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:  376.86 | Last Reward:  354.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 376.00 (±23.69)\n",
            "    Training Time: 0m 27s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 4/12: LR_3e3_hid_128\n",
            "======================================================================\n",
            "Learning Rate: 0.003, Hidden Dim: 128\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   41.00 | Last Reward:   41.00 | Steps: 350 | ETA:   0m 25s\n",
            "  Episode   50/858 | Recent Avg:  215.36 | Last Reward:  232.00 | Steps: 17,500 | ETA:   0m 28s\n",
            "  Episode  100/858 | Recent Avg:  242.76 | Last Reward:  215.00 | Steps: 35,000 | ETA:   0m 31s\n",
            "  Episode  150/858 | Recent Avg:  333.50 | Last Reward:  352.00 | Steps: 52,500 | ETA:   0m 26s\n",
            "  Episode  200/858 | Recent Avg:  357.78 | Last Reward:  373.00 | Steps: 70,000 | ETA:   0m 23s\n",
            "  Episode  250/858 | Recent Avg:  329.38 | Last Reward:  335.00 | Steps: 87,500 | ETA:   0m 21s\n",
            "  Episode  300/858 | Recent Avg:  366.30 | Last Reward:  397.00 | Steps: 105,000 | ETA:   0m 19s\n",
            "  Episode  350/858 | Recent Avg:  371.82 | Last Reward:  332.00 | Steps: 122,500 | ETA:   0m 17s\n",
            "  Episode  400/858 | Recent Avg:  375.08 | Last Reward:  376.00 | Steps: 140,000 | ETA:   0m 15s\n",
            "  Episode  450/858 | Recent Avg:  374.24 | Last Reward:  354.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:  379.96 | Last Reward:  354.00 | Steps: 175,000 | ETA:   0m 12s\n",
            "  Episode  550/858 | Recent Avg:  374.24 | Last Reward:  398.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:  379.08 | Last Reward:  376.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  374.68 | Last Reward:  398.00 | Steps: 227,500 | ETA:   0m 07s\n",
            "  Episode  700/858 | Recent Avg:  376.44 | Last Reward:  376.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  378.72 | Last Reward:  398.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  376.00 | Last Reward:  376.00 | Steps: 280,000 | ETA:   0m 02s\n",
            "  Episode  850/858 | Recent Avg:  375.12 | Last Reward:  376.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 376.00 (±23.69)\n",
            "    Training Time: 0m 30s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 5/12: LR_5e3_hid_64\n",
            "======================================================================\n",
            "Learning Rate: 0.005, Hidden Dim: 64\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:  145.00 | Last Reward:  145.00 | Steps: 350 | ETA:   0m 53s\n",
            "  Episode   50/858 | Recent Avg:  160.66 | Last Reward:  247.00 | Steps: 17,500 | ETA:   0m 28s\n",
            "  Episode  100/858 | Recent Avg:  309.38 | Last Reward:  316.00 | Steps: 35,000 | ETA:   0m 24s\n",
            "  Episode  150/858 | Recent Avg:  361.92 | Last Reward:  354.00 | Steps: 52,500 | ETA:   0m 22s\n",
            "  Episode  200/858 | Recent Avg:  379.00 | Last Reward:  376.00 | Steps: 70,000 | ETA:   0m 20s\n",
            "  Episode  250/858 | Recent Avg:  375.64 | Last Reward:  398.00 | Steps: 87,500 | ETA:   0m 18s\n",
            "  Episode  300/858 | Recent Avg:  377.84 | Last Reward:  354.00 | Steps: 105,000 | ETA:   0m 17s\n",
            "  Episode  350/858 | Recent Avg:  378.64 | Last Reward:  354.00 | Steps: 122,500 | ETA:   0m 15s\n",
            "  Episode  400/858 | Recent Avg:  373.78 | Last Reward:  376.00 | Steps: 140,000 | ETA:   0m 14s\n",
            "  Episode  450/858 | Recent Avg:  371.14 | Last Reward:  376.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:  382.60 | Last Reward:  376.00 | Steps: 175,000 | ETA:   0m 11s\n",
            "  Episode  550/858 | Recent Avg:  379.96 | Last Reward:  354.00 | Steps: 192,500 | ETA:   0m 09s\n",
            "  Episode  600/858 | Recent Avg:  378.62 | Last Reward:  354.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  373.36 | Last Reward:  376.00 | Steps: 227,500 | ETA:   0m 06s\n",
            "  Episode  700/858 | Recent Avg:  376.44 | Last Reward:  376.00 | Steps: 245,000 | ETA:   0m 04s\n",
            "  Episode  750/858 | Recent Avg:  379.96 | Last Reward:  376.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  370.38 | Last Reward:  354.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:  368.18 | Last Reward:  332.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 381.28 (±21.36)\n",
            "    Training Time: 0m 27s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 6/12: LR_5e3_hid_128\n",
            "======================================================================\n",
            "Learning Rate: 0.005, Hidden Dim: 128\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   88.00 | Last Reward:   88.00 | Steps: 350 | ETA:   0m 37s\n",
            "  Episode   50/858 | Recent Avg:   43.44 | Last Reward:   37.00 | Steps: 17,500 | ETA:   0m 25s\n",
            "  Episode  100/858 | Recent Avg:   41.80 | Last Reward:   37.00 | Steps: 35,000 | ETA:   0m 23s\n",
            "  Episode  150/858 | Recent Avg:   30.26 | Last Reward:   -3.00 | Steps: 52,500 | ETA:   0m 22s\n",
            "  Episode  200/858 | Recent Avg:   45.00 | Last Reward:   77.00 | Steps: 70,000 | ETA:   0m 20s\n",
            "  Episode  250/858 | Recent Avg:   33.80 | Last Reward:   37.00 | Steps: 87,500 | ETA:   0m 18s\n",
            "  Episode  300/858 | Recent Avg:   36.20 | Last Reward:  -43.00 | Steps: 105,000 | ETA:   0m 17s\n",
            "  Episode  350/858 | Recent Avg:   29.80 | Last Reward:   37.00 | Steps: 122,500 | ETA:   0m 16s\n",
            "  Episode  400/858 | Recent Avg:   38.60 | Last Reward:   -3.00 | Steps: 140,000 | ETA:   0m 15s\n",
            "  Episode  450/858 | Recent Avg:   35.40 | Last Reward:   37.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:   32.20 | Last Reward:   77.00 | Steps: 175,000 | ETA:   0m 11s\n",
            "  Episode  550/858 | Recent Avg:   42.60 | Last Reward:   37.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:   38.60 | Last Reward:   77.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:   37.00 | Last Reward:   -3.00 | Steps: 227,500 | ETA:   0m 06s\n",
            "  Episode  700/858 | Recent Avg:   35.40 | Last Reward:   -3.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:   33.00 | Last Reward:  117.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:   37.80 | Last Reward:   77.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:   46.60 | Last Reward:   77.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 45.00 (±37.52)\n",
            "    Training Time: 0m 28s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 7/12: LR_1e2_hid_64\n",
            "======================================================================\n",
            "Learning Rate: 0.01, Hidden Dim: 64\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:  151.00 | Last Reward:  151.00 | Steps: 350 | ETA:   0m 30s\n",
            "  Episode   50/858 | Recent Avg:  294.48 | Last Reward:  354.00 | Steps: 17,500 | ETA:   0m 24s\n",
            "  Episode  100/858 | Recent Avg:  278.60 | Last Reward:  260.00 | Steps: 35,000 | ETA:   0m 22s\n",
            "  Episode  150/858 | Recent Avg:  228.08 | Last Reward:  260.00 | Steps: 52,500 | ETA:   0m 20s\n",
            "  Episode  200/858 | Recent Avg:  231.44 | Last Reward:  288.00 | Steps: 70,000 | ETA:   0m 19s\n",
            "  Episode  250/858 | Recent Avg:  235.36 | Last Reward:  232.00 | Steps: 87,500 | ETA:   0m 17s\n",
            "  Episode  300/858 | Recent Avg:  231.44 | Last Reward:  232.00 | Steps: 105,000 | ETA:   0m 16s\n",
            "  Episode  350/858 | Recent Avg:  229.76 | Last Reward:  232.00 | Steps: 122,500 | ETA:   0m 15s\n",
            "  Episode  400/858 | Recent Avg:  238.72 | Last Reward:  232.00 | Steps: 140,000 | ETA:   0m 14s\n",
            "  Episode  450/858 | Recent Avg:  230.32 | Last Reward:  232.00 | Steps: 157,500 | ETA:   0m 12s\n",
            "  Episode  500/858 | Recent Avg:  230.32 | Last Reward:  204.00 | Steps: 175,000 | ETA:   0m 11s\n",
            "  Episode  550/858 | Recent Avg:  233.68 | Last Reward:  288.00 | Steps: 192,500 | ETA:   0m 09s\n",
            "  Episode  600/858 | Recent Avg:  236.48 | Last Reward:  232.00 | Steps: 210,000 | ETA:   0m 07s\n",
            "  Episode  650/858 | Recent Avg:  226.40 | Last Reward:  232.00 | Steps: 227,500 | ETA:   0m 06s\n",
            "  Episode  700/858 | Recent Avg:  238.72 | Last Reward:  260.00 | Steps: 245,000 | ETA:   0m 04s\n",
            "  Episode  750/858 | Recent Avg:  234.80 | Last Reward:  232.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  226.96 | Last Reward:  260.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:  233.12 | Last Reward:  260.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 237.04 (±24.21)\n",
            "    Training Time: 0m 27s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 8/12: LR_1e2_hid_128\n",
            "======================================================================\n",
            "Learning Rate: 0.01, Hidden Dim: 128\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   73.00 | Last Reward:   73.00 | Steps: 350 | ETA:   0m 27s\n",
            "  Episode   50/858 | Recent Avg:  276.58 | Last Reward:  388.00 | Steps: 17,500 | ETA:   0m 24s\n",
            "  Episode  100/858 | Recent Avg:  266.06 | Last Reward:  260.00 | Steps: 35,000 | ETA:   0m 22s\n",
            "  Episode  150/858 | Recent Avg:  228.64 | Last Reward:  204.00 | Steps: 52,500 | ETA:   0m 21s\n",
            "  Episode  200/858 | Recent Avg:  234.24 | Last Reward:  232.00 | Steps: 70,000 | ETA:   0m 19s\n",
            "  Episode  250/858 | Recent Avg:  240.96 | Last Reward:  232.00 | Steps: 87,500 | ETA:   0m 18s\n",
            "  Episode  300/858 | Recent Avg:  226.96 | Last Reward:  204.00 | Steps: 105,000 | ETA:   0m 17s\n",
            "  Episode  350/858 | Recent Avg:  234.80 | Last Reward:  204.00 | Steps: 122,500 | ETA:   0m 16s\n",
            "  Episode  400/858 | Recent Avg:  229.20 | Last Reward:  232.00 | Steps: 140,000 | ETA:   0m 14s\n",
            "  Episode  450/858 | Recent Avg:  233.12 | Last Reward:  204.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:  233.12 | Last Reward:  232.00 | Steps: 175,000 | ETA:   0m 11s\n",
            "  Episode  550/858 | Recent Avg:  235.36 | Last Reward:  176.00 | Steps: 192,500 | ETA:   0m 09s\n",
            "  Episode  600/858 | Recent Avg:  235.92 | Last Reward:  204.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  232.00 | Last Reward:  176.00 | Steps: 227,500 | ETA:   0m 06s\n",
            "  Episode  700/858 | Recent Avg:  225.28 | Last Reward:  176.00 | Steps: 245,000 | ETA:   0m 04s\n",
            "  Episode  750/858 | Recent Avg:  232.00 | Last Reward:  260.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  234.80 | Last Reward:  260.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:  236.48 | Last Reward:  204.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 236.48 (±28.20)\n",
            "    Training Time: 0m 27s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 9/12: LR_1e2_hid_256\n",
            "======================================================================\n",
            "Learning Rate: 0.01, Hidden Dim: 256\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   10.00 | Last Reward:   10.00 | Steps: 350 | ETA:   0m 26s\n",
            "  Episode   50/858 | Recent Avg:  234.00 | Last Reward:  312.00 | Steps: 17,500 | ETA:   0m 26s\n",
            "  Episode  100/858 | Recent Avg:  338.24 | Last Reward:  375.00 | Steps: 35,000 | ETA:   0m 25s\n",
            "  Episode  150/858 | Recent Avg:  352.00 | Last Reward:  329.00 | Steps: 52,500 | ETA:   0m 23s\n",
            "  Episode  200/858 | Recent Avg:  353.84 | Last Reward:  329.00 | Steps: 70,000 | ETA:   0m 21s\n",
            "  Episode  250/858 | Recent Avg:  354.76 | Last Reward:  375.00 | Steps: 87,500 | ETA:   0m 20s\n",
            "  Episode  300/858 | Recent Avg:  354.30 | Last Reward:  352.00 | Steps: 105,000 | ETA:   0m 19s\n",
            "  Episode  350/858 | Recent Avg:  352.46 | Last Reward:  398.00 | Steps: 122,500 | ETA:   0m 17s\n",
            "  Episode  400/858 | Recent Avg:  351.54 | Last Reward:  375.00 | Steps: 140,000 | ETA:   0m 15s\n",
            "  Episode  450/858 | Recent Avg:  357.06 | Last Reward:  375.00 | Steps: 157,500 | ETA:   0m 14s\n",
            "  Episode  500/858 | Recent Avg:  346.94 | Last Reward:  329.00 | Steps: 175,000 | ETA:   0m 12s\n",
            "  Episode  550/858 | Recent Avg:  352.92 | Last Reward:  352.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:  345.56 | Last Reward:  329.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  349.70 | Last Reward:  375.00 | Steps: 227,500 | ETA:   0m 07s\n",
            "  Episode  700/858 | Recent Avg:  349.24 | Last Reward:  352.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  349.70 | Last Reward:  306.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  355.22 | Last Reward:  352.00 | Steps: 280,000 | ETA:   0m 02s\n",
            "  Episode  850/858 | Recent Avg:  358.90 | Last Reward:  352.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 358.44 (±23.02)\n",
            "    Training Time: 0m 29s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 10/12: LR_5e3_hid_256\n",
            "======================================================================\n",
            "Learning Rate: 0.005, Hidden Dim: 256\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   42.00 | Last Reward:   42.00 | Steps: 350 | ETA:   0m 29s\n",
            "  Episode   50/858 | Recent Avg:  230.94 | Last Reward:  232.00 | Steps: 17,500 | ETA:   0m 25s\n",
            "  Episode  100/858 | Recent Avg:  230.88 | Last Reward:  204.00 | Steps: 35,000 | ETA:   0m 23s\n",
            "  Episode  150/858 | Recent Avg:  232.00 | Last Reward:  260.00 | Steps: 52,500 | ETA:   0m 23s\n",
            "  Episode  200/858 | Recent Avg:  234.24 | Last Reward:  232.00 | Steps: 70,000 | ETA:   0m 22s\n",
            "  Episode  250/858 | Recent Avg:  227.52 | Last Reward:  260.00 | Steps: 87,500 | ETA:   0m 22s\n",
            "  Episode  300/858 | Recent Avg:  234.24 | Last Reward:  288.00 | Steps: 105,000 | ETA:   0m 19s\n",
            "  Episode  350/858 | Recent Avg:  232.56 | Last Reward:  204.00 | Steps: 122,500 | ETA:   0m 17s\n",
            "  Episode  400/858 | Recent Avg:  230.88 | Last Reward:  232.00 | Steps: 140,000 | ETA:   0m 15s\n",
            "  Episode  450/858 | Recent Avg:  234.80 | Last Reward:  232.00 | Steps: 157,500 | ETA:   0m 13s\n",
            "  Episode  500/858 | Recent Avg:  230.88 | Last Reward:  232.00 | Steps: 175,000 | ETA:   0m 12s\n",
            "  Episode  550/858 | Recent Avg:  233.68 | Last Reward:  232.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:  233.12 | Last Reward:  232.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  234.80 | Last Reward:  204.00 | Steps: 227,500 | ETA:   0m 07s\n",
            "  Episode  700/858 | Recent Avg:  230.88 | Last Reward:  232.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  229.20 | Last Reward:  232.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  240.96 | Last Reward:  260.00 | Steps: 280,000 | ETA:   0m 02s\n",
            "  Episode  850/858 | Recent Avg:  231.44 | Last Reward:  232.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 225.84 (±28.16)\n",
            "    Training Time: 0m 29s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 11/12: LR_3e3_hid_256\n",
            "======================================================================\n",
            "Learning Rate: 0.003, Hidden Dim: 256\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   77.00 | Last Reward:   77.00 | Steps: 350 | ETA:   0m 43s\n",
            "  Episode   50/858 | Recent Avg:  257.12 | Last Reward:  322.00 | Steps: 17,500 | ETA:   0m 25s\n",
            "  Episode  100/858 | Recent Avg:  256.34 | Last Reward:  232.00 | Steps: 35,000 | ETA:   0m 26s\n",
            "  Episode  150/858 | Recent Avg:  231.00 | Last Reward:  232.00 | Steps: 52,500 | ETA:   0m 26s\n",
            "  Episode  200/858 | Recent Avg:  230.86 | Last Reward:  232.00 | Steps: 70,000 | ETA:   0m 24s\n",
            "  Episode  250/858 | Recent Avg:  224.38 | Last Reward:  193.00 | Steps: 87,500 | ETA:   0m 24s\n",
            "  Episode  300/858 | Recent Avg:  214.52 | Last Reward:  312.00 | Steps: 105,000 | ETA:   0m 21s\n",
            "  Episode  350/858 | Recent Avg:  345.42 | Last Reward:  329.00 | Steps: 122,500 | ETA:   0m 19s\n",
            "  Episode  400/858 | Recent Avg:  341.36 | Last Reward:  352.00 | Steps: 140,000 | ETA:   0m 17s\n",
            "  Episode  450/858 | Recent Avg:  352.46 | Last Reward:  375.00 | Steps: 157,500 | ETA:   0m 15s\n",
            "  Episode  500/858 | Recent Avg:  347.66 | Last Reward:  329.00 | Steps: 175,000 | ETA:   0m 13s\n",
            "  Episode  550/858 | Recent Avg:  349.88 | Last Reward:  329.00 | Steps: 192,500 | ETA:   0m 11s\n",
            "  Episode  600/858 | Recent Avg:  343.26 | Last Reward:  329.00 | Steps: 210,000 | ETA:   0m 09s\n",
            "  Episode  650/858 | Recent Avg:  355.68 | Last Reward:  375.00 | Steps: 227,500 | ETA:   0m 07s\n",
            "  Episode  700/858 | Recent Avg:  350.62 | Last Reward:  375.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  356.14 | Last Reward:  352.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  347.40 | Last Reward:  352.00 | Steps: 280,000 | ETA:   0m 02s\n",
            "  Episode  850/858 | Recent Avg:  357.52 | Last Reward:  306.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 354.30 (±22.18)\n",
            "    Training Time: 0m 31s\n",
            "\n",
            "======================================================================\n",
            "Training Configuration 12/12: LR_1e3_hid_256\n",
            "======================================================================\n",
            "Learning Rate: 0.001, Hidden Dim: 256\n",
            "Target: 300,000 timesteps (~858 episodes)\n",
            "Device: cpu\n",
            "\n",
            "  Episode    1/858 | Recent Avg:   54.00 | Last Reward:   54.00 | Steps: 350 | ETA:   0m 33s\n",
            "  Episode   50/858 | Recent Avg:  176.66 | Last Reward:  285.00 | Steps: 17,500 | ETA:   0m 31s\n",
            "  Episode  100/858 | Recent Avg:  296.38 | Last Reward:  314.00 | Steps: 35,000 | ETA:   0m 26s\n",
            "  Episode  150/858 | Recent Avg:  356.62 | Last Reward:  326.00 | Steps: 52,500 | ETA:   0m 24s\n",
            "  Episode  200/858 | Recent Avg:  371.74 | Last Reward:  392.00 | Steps: 70,000 | ETA:   0m 21s\n",
            "  Episode  250/858 | Recent Avg:  375.10 | Last Reward:  376.00 | Steps: 87,500 | ETA:   0m 20s\n",
            "  Episode  300/858 | Recent Avg:  370.24 | Last Reward:  424.00 | Steps: 105,000 | ETA:   0m 18s\n",
            "  Episode  350/858 | Recent Avg:  334.16 | Last Reward:  349.00 | Steps: 122,500 | ETA:   0m 16s\n",
            "  Episode  400/858 | Recent Avg:  348.20 | Last Reward:  334.00 | Steps: 140,000 | ETA:   0m 15s\n",
            "  Episode  450/858 | Recent Avg:  391.80 | Last Reward:  384.00 | Steps: 157,500 | ETA:   0m 14s\n",
            "  Episode  500/858 | Recent Avg:  393.04 | Last Reward:  414.00 | Steps: 175,000 | ETA:   0m 12s\n",
            "  Episode  550/858 | Recent Avg:  408.82 | Last Reward:  428.00 | Steps: 192,500 | ETA:   0m 10s\n",
            "  Episode  600/858 | Recent Avg:  396.88 | Last Reward:  366.00 | Steps: 210,000 | ETA:   0m 08s\n",
            "  Episode  650/858 | Recent Avg:  406.62 | Last Reward:  412.00 | Steps: 227,500 | ETA:   0m 07s\n",
            "  Episode  700/858 | Recent Avg:  412.28 | Last Reward:  424.00 | Steps: 245,000 | ETA:   0m 05s\n",
            "  Episode  750/858 | Recent Avg:  414.34 | Last Reward:  412.00 | Steps: 262,500 | ETA:   0m 03s\n",
            "  Episode  800/858 | Recent Avg:  400.04 | Last Reward:  378.00 | Steps: 280,000 | ETA:   0m 01s\n",
            "  Episode  850/858 | Recent Avg:  383.40 | Last Reward:  412.00 | Steps: 297,500 | ETA:   0m 00s\n",
            "\n",
            "  Evaluating on 50 episodes...\n",
            "  ✓ Evaluation Complete!\n",
            "    Mean Reward: 406.72 (±17.40)\n",
            "    Training Time: 0m 29s\n",
            "\n",
            "======================================================================\n",
            "ALL TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Best Configuration: LR_1e3_hid_64\n",
            "Best Mean Reward: 423.12\n",
            "Total Time: 0m\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "results = {}\n",
        "best_reward = -float('inf')\n",
        "best_config = None\n",
        "best_agent = None\n",
        "\n",
        "total_configs = len(reinforce_configs)\n",
        "state_dim = 14\n",
        "action_dim = 5\n",
        "\n",
        "# Training parameters\n",
        "target_steps = 300000\n",
        "steps_per_episode = 350\n",
        "episodes_per_config = (target_steps + steps_per_episode - 1) // steps_per_episode  # ~857 episodes\n",
        "\n",
        "for idx, config in enumerate(reinforce_configs, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training Configuration {idx}/{total_configs}: {config['name']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Learning Rate: {config['learning_rate']}, Hidden Dim: {config['hidden_dim']}\")\n",
        "    print(f\"Target: {target_steps:,} timesteps (~{episodes_per_config} episodes)\")\n",
        "    print(f\"Device: {device}\\n\")\n",
        "\n",
        "    # Create environment\n",
        "    env = DaladalaEnv()\n",
        "\n",
        "    # Initialize REINFORCE agent with GPU support\n",
        "    agent = REINFORCEAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        learning_rate=config['learning_rate'],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Training loop with verbose progress\n",
        "    start_time = time.time()\n",
        "    episode_rewards = []\n",
        "    total_steps = 0\n",
        "\n",
        "    for episode in range(episodes_per_config):\n",
        "        ep_reward = agent.train_episode(env)\n",
        "        episode_rewards.append(ep_reward)\n",
        "        total_steps += steps_per_episode\n",
        "\n",
        "        # Print progress every 50 episodes\n",
        "        if (episode + 1) % 50 == 0 or episode == 0:\n",
        "            recent_avg = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
        "            elapsed = time.time() - start_time\n",
        "            eps_per_sec = (episode + 1) / elapsed\n",
        "            eta_sec = (episodes_per_config - episode - 1) / eps_per_sec if eps_per_sec > 0 else 0\n",
        "\n",
        "            print(f\"  Episode {episode+1:4d}/{episodes_per_config} | \"\n",
        "                  f\"Recent Avg: {recent_avg:7.2f} | \"\n",
        "                  f\"Last Reward: {ep_reward:7.2f} | \"\n",
        "                  f\"Steps: {total_steps:,} | \"\n",
        "                  f\"ETA: {int(eta_sec//60):3d}m {int(eta_sec%60):02d}s\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate agent on 50 episodes with verbose feedback\n",
        "    print(f\"\\n  Evaluating on 50 episodes...\")\n",
        "    eval_start = time.time()\n",
        "    mean_reward, std_reward = agent.evaluate(env, n_episodes=50)\n",
        "    eval_time = time.time() - eval_start\n",
        "\n",
        "    results[config['name']] = {\n",
        "        'config': config,\n",
        "        'mean_reward': mean_reward,\n",
        "        'std_reward': std_reward,\n",
        "        'training_time': training_time,\n",
        "        'eval_time': eval_time\n",
        "    }\n",
        "\n",
        "    print(f\"  ✓ Evaluation Complete!\")\n",
        "    print(f\"    Mean Reward: {mean_reward:.2f} (±{std_reward:.2f})\")\n",
        "    print(f\"    Training Time: {int(training_time//60)}m {int(training_time%60)}s\")\n",
        "\n",
        "    # Track best model\n",
        "    if mean_reward > best_reward:\n",
        "        best_reward = mean_reward\n",
        "        best_config = config['name']\n",
        "        best_agent = agent\n",
        "        print(f\"    ★ NEW BEST MODEL! ★\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"ALL TRAINING COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Best Configuration: {best_config}\")\n",
        "print(f\"Best Mean Reward: {best_reward:.2f}\")\n",
        "print(f\"Total Time: {int((time.time() - start_time)//60)}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb17727a",
      "metadata": {
        "id": "eb17727a"
      },
      "source": [
        "## Section 7: Save Best Model and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a392cb7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a392cb7c",
        "outputId": "3e0abcc6-9481-49f8-889c-defc011d9ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Best model saved to: /content/drive/MyDrive/daladala_results/models/reinforce/best_reinforce\n",
            "✓ Results saved to: /content/drive/MyDrive/daladala_results/results/reinforce_results.json\n",
            "\n",
            "================================================================================\n",
            "RESULTS SUMMARY - All 12 REINFORCE Configurations\n",
            "================================================================================\n",
            "        Config Mean Reward Std Reward    LR  Hidden\n",
            " LR_1e3_hid_64      423.12      20.62 0.001      64\n",
            "LR_1e3_hid_128      402.62      22.73 0.001     128\n",
            " LR_3e3_hid_64      376.00      23.69 0.003      64\n",
            "LR_3e3_hid_128      376.00      23.69 0.003     128\n",
            " LR_5e3_hid_64      381.28      21.36 0.005      64\n",
            "LR_5e3_hid_128       45.00      37.52 0.005     128\n",
            " LR_1e2_hid_64      237.04      24.21 0.010      64\n",
            "LR_1e2_hid_128      236.48      28.20 0.010     128\n",
            "LR_1e2_hid_256      358.44      23.02 0.010     256\n",
            "LR_5e3_hid_256      225.84      28.16 0.005     256\n",
            "LR_3e3_hid_256      354.30      22.18 0.003     256\n",
            "LR_1e3_hid_256      406.72      17.40 0.001     256\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Save best model\n",
        "best_model_path = '/content/drive/MyDrive/daladala_results/models/reinforce/best_reinforce'\n",
        "best_agent.save(best_model_path)\n",
        "print(f\"✓ Best model saved to: {best_model_path}\")\n",
        "\n",
        "# Save results as JSON\n",
        "results_json_path = '/content/drive/MyDrive/daladala_results/results/reinforce_results.json'\n",
        "results_summary = {}\n",
        "for config_name, config_results in results.items():\n",
        "    results_summary[config_name] = {\n",
        "        'mean_reward': float(config_results['mean_reward']),\n",
        "        'std_reward': float(config_results['std_reward']),\n",
        "        'hyperparameters': {\n",
        "            'learning_rate': config_results['config']['learning_rate'],\n",
        "            'hidden_dim': config_results['config']['hidden_dim']\n",
        "        }\n",
        "    }\n",
        "\n",
        "with open(results_json_path, 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "print(f\"✓ Results saved to: {results_json_path}\")\n",
        "\n",
        "# Display results table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS SUMMARY - All 12 REINFORCE Configurations\")\n",
        "print(\"=\"*80)\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'Config': name,\n",
        "        'Mean Reward': f\"{results[name]['mean_reward']:.2f}\",\n",
        "        'Std Reward': f\"{results[name]['std_reward']:.2f}\",\n",
        "        'LR': results[name]['config']['learning_rate'],\n",
        "        'Hidden': results[name]['config']['hidden_dim']\n",
        "    }\n",
        "    for name in results.keys()\n",
        "])\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e4e24c",
      "metadata": {
        "id": "70e4e24c"
      },
      "source": [
        "## Section 8: Test Best Model on Sample Episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9311192a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9311192a",
        "outputId": "47e75b92-96f9-4092-c245-592450910768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing best model (LR_1e3_hid_64) on 5 sample episodes:\n",
            "============================================================\n",
            "Episode 1: Reward = 402.00 (Steps: 29)\n",
            "Episode 2: Reward = 446.00 (Steps: 29)\n",
            "Episode 3: Reward = 446.00 (Steps: 29)\n",
            "Episode 4: Reward = 424.00 (Steps: 29)\n",
            "Episode 5: Reward = 402.00 (Steps: 29)\n",
            "============================================================\n",
            "Sample Episodes Mean Reward: 424.00\n",
            "Sample Episodes Std Reward:  19.68\n",
            "\n",
            "✓ REINFORCE training and evaluation complete!\n",
            "✓ Models and results saved to Google Drive\n"
          ]
        }
      ],
      "source": [
        "# Test the best model on 5 sample episodes\n",
        "print(f\"\\nTesting best model ({best_config}) on 5 sample episodes:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "env = DaladalaEnv()\n",
        "episode_rewards = []\n",
        "\n",
        "for ep in range(5):\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done:\n",
        "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            probs = best_agent.policy(obs_tensor)\n",
        "            action = probs.argmax(dim=-1).item()\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    print(f\"Episode {ep+1}: Reward = {total_reward:.2f} (Steps: {steps})\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Sample Episodes Mean Reward: {np.mean(episode_rewards):.2f}\")\n",
        "print(f\"Sample Episodes Std Reward:  {np.std(episode_rewards):.2f}\")\n",
        "print(\"\\n✓ REINFORCE training and evaluation complete!\")\n",
        "print(f\"✓ Models and results saved to Google Drive\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
