{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b7bbd2",
   "metadata": {},
   "source": [
    "# REINFORCE Training on Google Colab\n",
    "\n",
    "This notebook contains a complete, self-contained REINFORCE (Policy Gradient) training pipeline for the Daladala environment.\n",
    "\n",
    "**What's Included:**\n",
    "- Full environment definition (5 actions, 14 observations)\n",
    "- 12 REINFORCE hyperparameter configurations for systematic tuning\n",
    "- Training loop with 300,000 timesteps per configuration\n",
    "- Automatic best model tracking and evaluation\n",
    "- Results saved directly to Google Drive\n",
    "\n",
    "**Expected Runtime:** ~3-4 hours on Colab CPU for all 12 configurations\n",
    "**Output:** Best model + detailed results JSON saved to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365ab01",
   "metadata": {},
   "source": [
    "## Section 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium torch pandas numpy opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163cd6a",
   "metadata": {},
   "source": [
    "## Section 2: Mount Google Drive (for saving models and results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/models/reinforce', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/results', exist_ok=True)\n",
    "print(\"✓ Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be0212",
   "metadata": {},
   "source": [
    "## Section 3: Define the DaladalaEnv Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaladalaEnv(gym.Env):\n",
    "    \"\"\"Daladala (mini-bus) optimization environment with 5 actions and 14 observations.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 12}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(-1, 1, shape=(14,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(5)  # Move, Stop, Pickup, Dropoff, SpeedUp\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Fixed route Ubungo → Posta (right then up)\n",
    "        self.route = [(x, 14) for x in range(15)] + [(14, y) for y in range(13, -1, -1)]\n",
    "        self.high_demand_stops = [(4,14), (8,14), (14,8), (14,3)]\n",
    "        self.police_checkpoints = [(6,14), (11,14), (14,10)]\n",
    "        self.traffic_lights = [(3,14), (10,14), (14,12), (14,5)]\n",
    "\n",
    "        self.max_steps = 350\n",
    "        self.physical_max = 50\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.passengers = 0\n",
    "        self.money = 0.0\n",
    "        self.pos_idx = 0\n",
    "        self.speed = 0\n",
    "        self.fined = False\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.pos_idx >= len(self.route):\n",
    "            x, y = 14, 0\n",
    "        else:\n",
    "            x, y = self.route[self.pos_idx]\n",
    "\n",
    "        # Get next cell info\n",
    "        next_idx = min(self.pos_idx + 1, len(self.route)-1)\n",
    "        next_x, next_y = self.route[next_idx]\n",
    "\n",
    "        # Traffic light logic\n",
    "        light_red = 0\n",
    "        if (x,y) in self.traffic_lights:\n",
    "            light_red = 1 if (self.step_count // 40) % 2 == 0 else 0\n",
    "\n",
    "        # Police & must_stop detection\n",
    "        police_checkpoint_ahead = 1 if (next_x, next_y) in self.police_checkpoints else 0\n",
    "        must_stop_next = 1 if police_checkpoint_ahead or (light_red and (x,y) in self.traffic_lights) else 0\n",
    "\n",
    "        # Calculate distances\n",
    "        dist_to_next_light = float('inf')\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.traffic_lights:\n",
    "                dist_to_next_light = i - self.pos_idx\n",
    "                break\n",
    "        dist_to_next_light = min(dist_to_next_light / 5.0, 1.0)\n",
    "\n",
    "        dist_to_next_police = float('inf')\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.police_checkpoints:\n",
    "                dist_to_next_police = i - self.pos_idx\n",
    "                break\n",
    "        dist_to_next_police = min(dist_to_next_police / 5.0, 1.0)\n",
    "\n",
    "        # Passengers waiting at high-demand stops\n",
    "        if not hasattr(self, 'passengers_waiting_state'):\n",
    "            self.passengers_waiting_state = {}\n",
    "        \n",
    "        if (x, y) in self.high_demand_stops:\n",
    "            if (x, y) not in self.passengers_waiting_state:\n",
    "                self.passengers_waiting_state[(x, y)] = np.random.randint(0, 11)\n",
    "        else:\n",
    "            self.passengers_waiting_state = {}\n",
    "        \n",
    "        passengers_waiting = self.passengers_waiting_state.get((x, y), 0) / 10.0\n",
    "\n",
    "        # Normalize all observations to [-1, 1]\n",
    "        obs = np.array([\n",
    "            x / 14.0 * 2 - 1,\n",
    "            y / 14.0 * 2 - 1,\n",
    "            self.passengers / 50.0 * 2 - 1,\n",
    "            self.money / 150000.0 * 2 - 1,\n",
    "            self.speed / 3.0,\n",
    "            dist_to_next_light * 2 - 1,\n",
    "            dist_to_next_police * 2 - 1,\n",
    "            light_red * 2 - 1,\n",
    "            must_stop_next * 2 - 1,\n",
    "            1 if (x,y) in self.high_demand_stops else -1,\n",
    "            passengers_waiting * 2 - 1,\n",
    "            1 if self.passengers > 40 else -1,\n",
    "            1 if self.fined else -1,\n",
    "            self.step_count / self.max_steps * 2 - 1\n",
    "        ], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        terminated = truncated = False\n",
    "\n",
    "        x, y = self.route[self.pos_idx]\n",
    "        must_stop_now = self._must_stop_here()\n",
    "\n",
    "        # Capture pre-action state\n",
    "        prev_passengers = self.passengers\n",
    "        prev_money = self.money\n",
    "        prev_pos_idx = self.pos_idx\n",
    "\n",
    "        # Execute action (pure state transitions, no conditions)\n",
    "        if action == 0:  # Move Forward\n",
    "            if self.pos_idx < len(self.route) - 1:\n",
    "                self.pos_idx += 1\n",
    "                self.speed = min(self.speed + 1, 3)\n",
    "            else:\n",
    "                terminated = True\n",
    "\n",
    "        elif action == 1:  # Stop\n",
    "            self.speed = 0\n",
    "\n",
    "        elif action == 2:  # Pick up passengers\n",
    "            if (x, y) in self.high_demand_stops and self.passengers < self.physical_max:\n",
    "                base_add = np.random.randint(4, 9)\n",
    "                waiting_count = int(self.passengers_waiting_state.get((x, y), 0))\n",
    "                add = min(base_add + waiting_count // 2, self.physical_max - self.passengers)\n",
    "                self.passengers = min(self.passengers + add, self.physical_max)\n",
    "                if (x, y) in self.passengers_waiting_state:\n",
    "                    self.passengers_waiting_state[(x, y)] = max(0, waiting_count - add)\n",
    "\n",
    "        elif action == 3:  # Drop off passengers\n",
    "            if (x, y) in self.high_demand_stops and self.passengers > 0:\n",
    "                drop = min(self.passengers, np.random.randint(6, 16))\n",
    "                self.passengers -= drop\n",
    "                self.money += drop * 1000\n",
    "\n",
    "        elif action == 4:  # Speed Up\n",
    "            if self.passengers <= 40:\n",
    "                self.speed = min(self.speed + 1, 3)\n",
    "\n",
    "        # Measure outcomes (pure state deltas)\n",
    "        pos_progress = self.pos_idx - prev_pos_idx\n",
    "        passengers_added = self.passengers - prev_passengers\n",
    "        passengers_dropped = prev_passengers - self.passengers\n",
    "        money_earned = self.money - prev_money\n",
    "\n",
    "        # Calculate reward from outcomes only\n",
    "        reward = 0.0\n",
    "\n",
    "        if pos_progress > 0:\n",
    "            reward += 5\n",
    "        \n",
    "        if passengers_added > 0:\n",
    "            reward += passengers_added * 1.0\n",
    "        \n",
    "        if passengers_dropped > 0:\n",
    "            reward += passengers_dropped * 1.2\n",
    "        \n",
    "        if money_earned > 0:\n",
    "            reward += money_earned / 20000.0\n",
    "\n",
    "        if terminated:\n",
    "            reward += 100\n",
    "            if self.passengers <= 33:\n",
    "                reward += 200\n",
    "\n",
    "        # Safety consequences\n",
    "        if action == 0 and must_stop_now:\n",
    "            reward -= 45\n",
    "        \n",
    "        if action == 1 and not must_stop_now:\n",
    "            reward -= 2\n",
    "        \n",
    "        if action == 1 and must_stop_now:\n",
    "            reward += 6\n",
    "        \n",
    "        if action == 4 and self.passengers > 40:\n",
    "            reward -= 400\n",
    "            terminated = True\n",
    "        \n",
    "        if (x, y) in self.police_checkpoints:\n",
    "            if self.passengers > 40:\n",
    "                reward -= 200\n",
    "                terminated = True\n",
    "                self.fined = True\n",
    "            elif self.passengers > 33:\n",
    "                reward -= 40\n",
    "                self.fined = True\n",
    "\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "    def _must_stop_here(self):\n",
    "        if self.pos_idx >= len(self.route)-1:\n",
    "            return False\n",
    "        nx, ny = self.route[self.pos_idx + 1]\n",
    "        light_red = 0\n",
    "        cx, cy = self.route[self.pos_idx]\n",
    "        if (cx,cy) in self.traffic_lights:\n",
    "            light_red = 1 if (self.step_count // 40) % 2 == 0 else 0\n",
    "        return (nx,ny) in self.police_checkpoints or light_red == 1\n",
    "\n",
    "    def render(self):\n",
    "        pass  # Rendering disabled for Colab\n",
    "\n",
    "print(\"✓ DaladalaEnv class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce5542",
   "metadata": {},
   "source": [
    "## Section 4: Define REINFORCE Policy Network and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbee83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for REINFORCE algorithm.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return torch.softmax(self.net(state), dim=-1)\n",
    "    \n",
    "    def get_action_and_log_prob(self, state):\n",
    "        \"\"\"Get action and log probability from policy.\"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE (Policy Gradient) agent.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, learning_rate):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device('cpu')\n",
    "        self.policy.to(self.device)\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Train for one complete episode.\"\"\"\n",
    "        obs, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "            action, log_prob = self.policy.get_action_and_log_prob(obs_tensor)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Calculate returns (discounted cumulative rewards)\n",
    "        returns = []\n",
    "        cumulative_return = 0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_return = reward + 0.99 * cumulative_return\n",
    "            returns.insert(0, cumulative_return)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = 0\n",
    "        for log_prob, return_val in zip(log_probs, returns):\n",
    "            policy_loss += -log_prob * return_val\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return sum(rewards)\n",
    "    \n",
    "    def evaluate(self, env, n_episodes=50):\n",
    "        \"\"\"Evaluate agent performance.\"\"\"\n",
    "        rewards = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    probs = self.policy(obs_tensor)\n",
    "                    action = probs.argmax(dim=-1).item()\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "        \n",
    "        return np.mean(rewards), np.std(rewards)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        torch.save(self.policy.state_dict(), path + '_policy.pth')\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path + '_policy.pth'))\n",
    "\n",
    "print(\"✓ REINFORCE Policy Network and Agent defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01559b15",
   "metadata": {},
   "source": [
    "## Section 5: Define REINFORCE Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 REINFORCE hyperparameter configurations for systematic tuning\n",
    "reinforce_configs = [\n",
    "    {\"name\": \"LR_1e3_hid_64\", \"learning_rate\": 1e-3, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_1e3_hid_128\", \"learning_rate\": 1e-3, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_3e3_hid_64\", \"learning_rate\": 3e-3, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_3e3_hid_128\", \"learning_rate\": 3e-3, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_5e3_hid_64\", \"learning_rate\": 5e-3, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_5e3_hid_128\", \"learning_rate\": 5e-3, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_1e2_hid_64\", \"learning_rate\": 1e-2, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_1e2_hid_128\", \"learning_rate\": 1e-2, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_1e2_hid_256\", \"learning_rate\": 1e-2, \"hidden_dim\": 256},\n",
    "    {\"name\": \"LR_5e3_hid_256\", \"learning_rate\": 5e-3, \"hidden_dim\": 256},\n",
    "    {\"name\": \"LR_3e3_hid_256\", \"learning_rate\": 3e-3, \"hidden_dim\": 256},\n",
    "    {\"name\": \"LR_1e3_hid_256\", \"learning_rate\": 1e-3, \"hidden_dim\": 256},\n",
    "]\n",
    "\n",
    "print(f\"✓ {len(reinforce_configs)} REINFORCE configurations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581dc93",
   "metadata": {},
   "source": [
    "## Section 6: Train REINFORCE with All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe93480",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_reward = -float('inf')\n",
    "best_config = None\n",
    "best_agent = None\n",
    "\n",
    "total_configs = len(reinforce_configs)\n",
    "state_dim = 14\n",
    "action_dim = 5\n",
    "\n",
    "for idx, config in enumerate(reinforce_configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Configuration {idx}/{total_configs}: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = DaladalaEnv()\n",
    "    \n",
    "    # Initialize REINFORCE agent\n",
    "    agent = REINFORCEAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Train agent - convert 300k timesteps to approximate episodes\n",
    "    # Typical episode length ~350 steps, so ~857 episodes for 300k timesteps\n",
    "    print(f\"Training for ~857 episodes (~300k timesteps)...\")\n",
    "    total_steps = 0\n",
    "    target_steps = 300000\n",
    "    \n",
    "    while total_steps < target_steps:\n",
    "        ep_reward = agent.train_episode(env)\n",
    "        total_steps += 350  # Approximate steps per episode\n",
    "    \n",
    "    # Evaluate agent on 50 episodes\n",
    "    print(f\"Evaluating on 50 episodes...\")\n",
    "    mean_reward, std_reward = agent.evaluate(env, n_episodes=50)\n",
    "    \n",
    "    results[config['name']] = {\n",
    "        'config': config,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Mean Reward: {mean_reward:.2f} (±{std_reward:.2f})\")\n",
    "    \n",
    "    # Track best model\n",
    "    if mean_reward > best_reward:\n",
    "        best_reward = mean_reward\n",
    "        best_config = config['name']\n",
    "        best_agent = agent\n",
    "        print(f\"★ New best model!\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Configuration: {best_config}\")\n",
    "print(f\"Best Mean Reward: {best_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17727a",
   "metadata": {},
   "source": [
    "## Section 7: Save Best Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_path = '/content/drive/MyDrive/daladala_results/models/reinforce/best_reinforce'\n",
    "best_agent.save(best_model_path)\n",
    "print(f\"✓ Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Save results as JSON\n",
    "results_json_path = '/content/drive/MyDrive/daladala_results/results/reinforce_results.json'\n",
    "results_summary = {}\n",
    "for config_name, config_results in results.items():\n",
    "    results_summary[config_name] = {\n",
    "        'mean_reward': float(config_results['mean_reward']),\n",
    "        'std_reward': float(config_results['std_reward']),\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': config_results['config']['learning_rate'],\n",
    "            'hidden_dim': config_results['config']['hidden_dim']\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"✓ Results saved to: {results_json_path}\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY - All 12 REINFORCE Configurations\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Config': name,\n",
    "        'Mean Reward': f\"{results[name]['mean_reward']:.2f}\",\n",
    "        'Std Reward': f\"{results[name]['std_reward']:.2f}\",\n",
    "        'LR': results[name]['config']['learning_rate'],\n",
    "        'Hidden': results[name]['config']['hidden_dim']\n",
    "    }\n",
    "    for name in results.keys()\n",
    "])\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4e24c",
   "metadata": {},
   "source": [
    "## Section 8: Test Best Model on Sample Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model on 5 sample episodes\n",
    "print(f\"\\nTesting best model ({best_config}) on 5 sample episodes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = DaladalaEnv()\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            probs = best_agent.policy(obs_tensor)\n",
    "            action = probs.argmax(dim=-1).item()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1}: Reward = {total_reward:.2f} (Steps: {steps})\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample Episodes Mean Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Sample Episodes Std Reward:  {np.std(episode_rewards):.2f}\")\n",
    "print(\"\\n✓ REINFORCE training and evaluation complete!\")\n",
    "print(f\"✓ Models and results saved to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
