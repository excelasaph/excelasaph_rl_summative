{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b7bbd2",
   "metadata": {},
   "source": [
    "# REINFORCE Training on Google Colab\n",
    "\n",
    "This notebook contains a complete, self-contained REINFORCE (Policy Gradient) training pipeline for the Daladala environment.\n",
    "\n",
    "**What's Included:**\n",
    "- Full environment definition (5 actions, 14 observations)\n",
    "- 12 REINFORCE hyperparameter configurations for systematic tuning\n",
    "- Training loop with 300,000 timesteps per configuration\n",
    "- Automatic best model tracking and evaluation\n",
    "- Results saved directly to Google Drive\n",
    "\n",
    "**Expected Runtime:** ~3-4 hours on Colab CPU for all 12 configurations\n",
    "**Output:** Best model + detailed results JSON saved to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365ab01",
   "metadata": {},
   "source": [
    "## Section 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium torch pandas numpy opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163cd6a",
   "metadata": {},
   "source": [
    "## Section 2: Mount Google Drive (for saving models and results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/models/reinforce', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/daladala_results/results', exist_ok=True)\n",
    "print(\"✓ Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be0212",
   "metadata": {},
   "source": [
    "## Section 3: Define the DaladalaEnv Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaladalaEnv(gym.Env):\n",
    "    \"\"\"Daladala (mini-bus) optimization environment with 5 actions and 14 observations.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 12}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(-1, 1, shape=(14,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(5)  # 0:Move, 1:Pickup, 2:Dropoff, 3:Stop, 4:SpeedUp\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Fixed route Ubungo → Posta (right then up)\n",
    "        self.route = [(x, 14) for x in range(15)] + [(14, y) for y in range(13, -1, -1)]\n",
    "        self.high_demand_stops = [(4,14), (8,14), (14,8), (14,3)]\n",
    "        \n",
    "        # These will be randomized each reset\n",
    "        self.police_checkpoints = []\n",
    "        self.traffic_lights = []\n",
    "        self.traffic_light_states = {}  # Stores state (Red=1, Green=0) for each light\n",
    "        \n",
    "        # Pools for randomization\n",
    "        self.available_positions = [pos for pos in self.route if pos not in self.high_demand_stops]\n",
    "\n",
    "        self.max_steps = 350\n",
    "        self.physical_max = 50\n",
    "        self.light_cycle = 0  # Track light cycle deterministically\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.passengers = 0\n",
    "        self.money = 0.0\n",
    "        self.pos_idx = 0\n",
    "        self.speed = 0\n",
    "        self.fined = False\n",
    "        self.light_cycle = 0\n",
    "        \n",
    "        # Randomize hazard positions each episode\n",
    "        available = [pos for pos in self.route if pos not in self.high_demand_stops]\n",
    "        if len(available) >= 7:  # 3 police + 4 traffic lights\n",
    "            sampled = np.random.choice(len(available), 7, replace=False)\n",
    "            self.police_checkpoints = [available[i] for i in sampled[:3]]\n",
    "            self.traffic_lights = [available[i] for i in sampled[3:7]]\n",
    "            \n",
    "            # Assign random constant state (Red=1, Green=0) for each light this episode\n",
    "            # This ensures the light stays the same throughout the episode\n",
    "            self.traffic_light_states = {pos: np.random.randint(0, 2) for pos in self.traffic_lights}\n",
    "        \n",
    "        # Initialize deterministic passenger counts per stop (seeded by position)\n",
    "        self.passengers_at_stop = {}\n",
    "        for stop in self.high_demand_stops:\n",
    "            # Deterministic: same stop always has same initial count\n",
    "            seed_val = hash(stop) % 11  # 0-10 passengers\n",
    "            self.passengers_at_stop[stop] = seed_val\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Generate observation based on current location and environment state.\n",
    "        Observations are DETERMINISTIC per location to match training/visualization.\n",
    "        \"\"\"\n",
    "        if self.pos_idx >= len(self.route):\n",
    "            x, y = 14, 0\n",
    "        else:\n",
    "            x, y = self.route[self.pos_idx]\n",
    "\n",
    "        # === CURRENT LOCATION HAZARDS ===\n",
    "        # Traffic light: Constant state for this episode (Red=1, Green=0)\n",
    "        light_is_red = self.traffic_light_states.get((x, y), 0)\n",
    "        \n",
    "        # Police checkpoint detection\n",
    "        police_here = 1 if (x, y) in self.police_checkpoints else 0\n",
    "        \n",
    "        # Must stop at THIS location?\n",
    "        must_stop_now = 1 if (light_is_red or police_here) else 0\n",
    "        \n",
    "        # === NEXT LOCATION HAZARDS ===\n",
    "        next_idx = min(self.pos_idx + 1, len(self.route) - 1)\n",
    "        next_x, next_y = self.route[next_idx]\n",
    "        \n",
    "        # Check what's ahead\n",
    "        next_light_is_red = self.traffic_light_states.get((next_x, next_y), 0)\n",
    "        police_ahead = 1 if (next_x, next_y) in self.police_checkpoints else 0\n",
    "        must_stop_next = 1 if (next_light_is_red or police_ahead) else 0\n",
    "        \n",
    "        # === PASSENGER STATE ===\n",
    "        # At high-demand stop: passengers waiting (deterministic)\n",
    "        at_stop = 1 if (x, y) in self.high_demand_stops else 0\n",
    "        passengers_waiting = self.passengers_at_stop.get((x, y), 0) if at_stop else 0\n",
    "        \n",
    "        # === DISTANCE AHEAD (for lookahead) ===\n",
    "        # Distance to next traffic light (in next 5 cells)\n",
    "        dist_to_light = 5\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.traffic_lights:\n",
    "                dist_to_light = i - self.pos_idx\n",
    "                break\n",
    "        \n",
    "        # Distance to next police (in next 5 cells)\n",
    "        dist_to_police = 5\n",
    "        for i in range(self.pos_idx + 1, min(self.pos_idx + 6, len(self.route))):\n",
    "            if self.route[i] in self.police_checkpoints:\n",
    "                dist_to_police = i - self.pos_idx\n",
    "                break\n",
    "        \n",
    "        # === BUILD OBSERVATION VECTOR (all normalized to [-1, 1]) ===\n",
    "        obs = np.array([\n",
    "            x / 14.0 * 2 - 1,                      # [0] position_x\n",
    "            y / 14.0 * 2 - 1,                      # [1] position_y\n",
    "            self.passengers / self.physical_max * 2 - 1,  # [2] current_passengers\n",
    "            self.money / 150000.0 * 2 - 1,        # [3] money_earned\n",
    "            self.speed / 3.0 * 2 - 1,              # [4] current_speed\n",
    "            light_is_red * 2 - 1,                  # [5] light_is_red_HERE (critical)\n",
    "            police_here * 2 - 1,                   # [6] police_checkpoint_HERE\n",
    "            must_stop_now * 2 - 1,                 # [7] must_stop_now_HERE (critical)\n",
    "            at_stop * 2 - 1,                       # [8] at_high_demand_stop\n",
    "            passengers_waiting / 10.0 * 2 - 1,    # [9] passengers_waiting_at_stop\n",
    "            must_stop_next * 2 - 1,                # [10] must_stop_next_location\n",
    "            dist_to_light / 5.0 * 2 - 1,          # [11] distance_to_traffic_light\n",
    "            dist_to_police / 5.0 * 2 - 1,         # [12] distance_to_police\n",
    "            self.step_count / self.max_steps * 2 - 1,  # [13] episode_progress\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Action: 0=Move, 1=Pickup, 2=Dropoff, 3=Stop, 4=SpeedUp\n",
    "        Movement is ALWAYS automatic. Actions are overlaid on movement.\n",
    "        Rewards guide agent toward optimal actions based on current state.\n",
    "        \"\"\"\n",
    "        self.step_count += 1\n",
    "        self.light_cycle += 1  # Update traffic light cycle\n",
    "        \n",
    "        terminated = truncated = False\n",
    "        x, y = self.route[self.pos_idx]\n",
    "        \n",
    "        # === PHASE 1: AUTOMATIC MOVEMENT (always happens) ===\n",
    "        if self.pos_idx < len(self.route) - 1:\n",
    "            self.pos_idx += 1\n",
    "        else:\n",
    "            terminated = True\n",
    "        \n",
    "        # === PHASE 2: EXECUTE ACTION ===\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Observe the CURRENT location (before action)\n",
    "        light_is_red = self.traffic_light_states.get((x, y), 0)\n",
    "        police_here = 1 if (x, y) in self.police_checkpoints else 0\n",
    "        must_stop_here = 1 if (light_is_red or police_here) else 0\n",
    "        at_stop = 1 if (x, y) in self.high_demand_stops else 0\n",
    "        \n",
    "        # === INTELLIGENT REWARD SYSTEM ===\n",
    "        # We know the \"right\" action for each state, so rewards guide strongly\n",
    "        \n",
    "        if action == 0:  # MOVE action (advance to next cell)\n",
    "            # Movement already happened automatically\n",
    "            # This action is mostly for consistency; reward small progress bonus\n",
    "            reward += 2\n",
    "            \n",
    "            # PENALTY: Moved through hazard without stopping\n",
    "            if must_stop_here:\n",
    "                reward -= 40  # Heavy penalty: ran through red light or police checkpoint\n",
    "        \n",
    "        elif action == 1:  # PICKUP action\n",
    "            if at_stop and self.passengers < self.physical_max:\n",
    "                # GOOD: Picking up at a stop\n",
    "                base_add = max(3, self.passengers_at_stop.get((x, y), 0))\n",
    "                add = min(base_add, self.physical_max - self.passengers)\n",
    "                self.passengers += add\n",
    "                reward += 15  # High reward for correct action\n",
    "                \n",
    "                # Deduct waiting passengers\n",
    "                if (x, y) in self.passengers_at_stop:\n",
    "                    self.passengers_at_stop[(x, y)] = max(0, self.passengers_at_stop[(x, y)] - add)\n",
    "            else:\n",
    "                # BAD: Picked up when not at stop\n",
    "                reward -= 5\n",
    "            \n",
    "            # PENALTY: Picking up at hazard zone\n",
    "            if must_stop_here:\n",
    "                reward -= 10\n",
    "        \n",
    "        elif action == 2:  # DROPOFF action\n",
    "            if at_stop and self.passengers > 0:\n",
    "                # GOOD: Dropping off at a stop\n",
    "                drop = min(self.passengers, max(3, self.passengers // 2 + 1))\n",
    "                self.passengers -= drop\n",
    "                self.money += drop * 1000\n",
    "                reward += 12  # Good reward for revenue\n",
    "            else:\n",
    "                # BAD: Dropped off when not at stop\n",
    "                reward -= 8\n",
    "            \n",
    "            # PENALTY: Dropping off at hazard zone\n",
    "            if must_stop_here:\n",
    "                reward -= 10\n",
    "        \n",
    "        elif action == 3:  # STOP action (slows down / waits)\n",
    "            self.speed = max(0, self.speed - 1)\n",
    "            \n",
    "            # GOOD: Stopped at hazard location\n",
    "            if must_stop_here:\n",
    "                reward += 25  # Strong reward: correct safety action\n",
    "            else:\n",
    "                # BAD: Unnecessary stop\n",
    "                reward -= 3\n",
    "        \n",
    "        elif action == 4:  # SPEEDUP action\n",
    "            # GOOD: Speeding up in safe zones\n",
    "            if not must_stop_here and self.passengers <= 40:\n",
    "                self.speed = min(self.speed + 1, 3)\n",
    "                reward += 3\n",
    "            else:\n",
    "                # BAD: Speeding in danger or when overloaded\n",
    "                if must_stop_here:\n",
    "                    reward -= 15  # Dangerous\n",
    "                if self.passengers > 40:\n",
    "                    reward -= 30  # Could crash\n",
    "                    terminated = True  # Crash!\n",
    "        \n",
    "        # === PHASE 3: SAFETY VIOLATIONS ===\n",
    "        # Check destination after automatic movement\n",
    "        new_x, new_y = self.route[self.pos_idx] if self.pos_idx < len(self.route) else (14, 0)\n",
    "        \n",
    "        # Police checkpoint consequences\n",
    "        if (new_x, new_y) in self.police_checkpoints:\n",
    "            if self.passengers > 40:\n",
    "                reward -= 50  # Severe: overloaded at police\n",
    "                self.fined = True\n",
    "                terminated = True\n",
    "            elif self.passengers > 33:\n",
    "                reward -= 20  # Violation: illegal capacity\n",
    "                self.fined = True\n",
    "        \n",
    "        # === PHASE 4: PROGRESS & COMPLETION ===\n",
    "        # Base movement reward (small, to encourage progress)\n",
    "        reward += 1\n",
    "        \n",
    "        # Destination completion bonus\n",
    "        if terminated:\n",
    "            reward += 100  # Large bonus for reaching destination\n",
    "            if self.passengers <= 33 and not self.fined:\n",
    "                reward += 50  # Bonus for legal completion\n",
    "        \n",
    "        # === PHASE 5: STATE UPDATES ===\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass  # Rendering disabled for Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce5542",
   "metadata": {},
   "source": [
    "## Section 4: Define REINFORCE Policy Network and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbee83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for REINFORCE algorithm.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return torch.softmax(self.net(state), dim=-1)\n",
    "    \n",
    "    def get_action_and_log_prob(self, state):\n",
    "        \"\"\"Get action and log probability from policy.\"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE (Policy Gradient) agent.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, learning_rate, device='cpu'):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.policy.to(self.device)\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Train for one complete episode.\"\"\"\n",
    "        obs, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "            action, log_prob = self.policy.get_action_and_log_prob(obs_tensor)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Calculate returns (discounted cumulative rewards)\n",
    "        returns = []\n",
    "        cumulative_return = 0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_return = reward + 0.99 * cumulative_return\n",
    "            returns.insert(0, cumulative_return)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = 0\n",
    "        for log_prob, return_val in zip(log_probs, returns):\n",
    "            policy_loss += -log_prob * return_val\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return sum(rewards)\n",
    "    \n",
    "    def evaluate(self, env, n_episodes=50):\n",
    "        \"\"\"Evaluate agent performance.\"\"\"\n",
    "        rewards = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    probs = self.policy(obs_tensor)\n",
    "                    action = probs.argmax(dim=-1).item()\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "        \n",
    "        return np.mean(rewards), np.std(rewards)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        torch.save(self.policy.state_dict(), path + '_policy.pth')\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path + '_policy.pth', map_location=self.device))\n",
    "\n",
    "print(\"✓ REINFORCE Policy Network and Agent defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01559b15",
   "metadata": {},
   "source": [
    "## Section 5: Define REINFORCE Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 REINFORCE hyperparameter configurations for systematic tuning\n",
    "reinforce_configs = [\n",
    "    {\"name\": \"LR_1e3_hid_64\", \"learning_rate\": 1e-3, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_1e3_hid_128\", \"learning_rate\": 1e-3, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_3e3_hid_64\", \"learning_rate\": 3e-3, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_3e3_hid_128\", \"learning_rate\": 3e-3, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_5e3_hid_64\", \"learning_rate\": 5e-3, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_5e3_hid_128\", \"learning_rate\": 5e-3, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_1e2_hid_64\", \"learning_rate\": 1e-2, \"hidden_dim\": 64},\n",
    "    {\"name\": \"LR_1e2_hid_128\", \"learning_rate\": 1e-2, \"hidden_dim\": 128},\n",
    "    {\"name\": \"LR_1e2_hid_256\", \"learning_rate\": 1e-2, \"hidden_dim\": 256},\n",
    "    {\"name\": \"LR_5e3_hid_256\", \"learning_rate\": 5e-3, \"hidden_dim\": 256},\n",
    "    {\"name\": \"LR_3e3_hid_256\", \"learning_rate\": 3e-3, \"hidden_dim\": 256},\n",
    "    {\"name\": \"LR_1e3_hid_256\", \"learning_rate\": 1e-3, \"hidden_dim\": 256},\n",
    "]\n",
    "\n",
    "print(f\"✓ {len(reinforce_configs)} REINFORCE configurations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581dc93",
   "metadata": {},
   "source": [
    "## Section 6: Train REINFORCE with All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe93480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results = {}\n",
    "best_reward = -float('inf')\n",
    "best_config = None\n",
    "best_agent = None\n",
    "\n",
    "total_configs = len(reinforce_configs)\n",
    "state_dim = 14\n",
    "action_dim = 5\n",
    "\n",
    "# Training parameters\n",
    "target_steps = 300000\n",
    "steps_per_episode = 350\n",
    "episodes_per_config = (target_steps + steps_per_episode - 1) // steps_per_episode  # ~857 episodes\n",
    "\n",
    "for idx, config in enumerate(reinforce_configs, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Configuration {idx}/{total_configs}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Learning Rate: {config['learning_rate']}, Hidden Dim: {config['hidden_dim']}\")\n",
    "    print(f\"Target: {target_steps:,} timesteps (~{episodes_per_config} episodes)\")\n",
    "    print(f\"Device: {device}\\n\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = DaladalaEnv()\n",
    "    \n",
    "    # Initialize REINFORCE agent with GPU support\n",
    "    agent = REINFORCEAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Training loop with verbose progress\n",
    "    start_time = time.time()\n",
    "    episode_rewards = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(episodes_per_config):\n",
    "        ep_reward = agent.train_episode(env)\n",
    "        episode_rewards.append(ep_reward)\n",
    "        total_steps += steps_per_episode\n",
    "        \n",
    "        # Print progress every 50 episodes\n",
    "        if (episode + 1) % 50 == 0 or episode == 0:\n",
    "            recent_avg = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
    "            elapsed = time.time() - start_time\n",
    "            eps_per_sec = (episode + 1) / elapsed\n",
    "            eta_sec = (episodes_per_config - episode - 1) / eps_per_sec if eps_per_sec > 0 else 0\n",
    "            \n",
    "            print(f\"  Episode {episode+1:4d}/{episodes_per_config} | \"\n",
    "                  f\"Recent Avg: {recent_avg:7.2f} | \"\n",
    "                  f\"Last Reward: {ep_reward:7.2f} | \"\n",
    "                  f\"Steps: {total_steps:,} | \"\n",
    "                  f\"ETA: {int(eta_sec//60):3d}m {int(eta_sec%60):02d}s\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate agent on 50 episodes with verbose feedback\n",
    "    print(f\"\\n  Evaluating on 50 episodes...\")\n",
    "    eval_start = time.time()\n",
    "    mean_reward, std_reward = agent.evaluate(env, n_episodes=50)\n",
    "    eval_time = time.time() - eval_start\n",
    "    \n",
    "    results[config['name']] = {\n",
    "        'config': config,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'training_time': training_time,\n",
    "        'eval_time': eval_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Evaluation Complete!\")\n",
    "    print(f\"    Mean Reward: {mean_reward:.2f} (±{std_reward:.2f})\")\n",
    "    print(f\"    Training Time: {int(training_time//60)}m {int(training_time%60)}s\")\n",
    "    \n",
    "    # Track best model\n",
    "    if mean_reward > best_reward:\n",
    "        best_reward = mean_reward\n",
    "        best_config = config['name']\n",
    "        best_agent = agent\n",
    "        print(f\"    ★ NEW BEST MODEL! ★\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Configuration: {best_config}\")\n",
    "print(f\"Best Mean Reward: {best_reward:.2f}\")\n",
    "print(f\"Total Time: {int((time.time() - start_time)//60)}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17727a",
   "metadata": {},
   "source": [
    "## Section 7: Save Best Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_path = '/content/drive/MyDrive/daladala_results/models/reinforce/best_reinforce'\n",
    "best_agent.save(best_model_path)\n",
    "print(f\"✓ Best model saved to: {best_model_path}\")\n",
    "\n",
    "# Save results as JSON\n",
    "results_json_path = '/content/drive/MyDrive/daladala_results/results/reinforce_results.json'\n",
    "results_summary = {}\n",
    "for config_name, config_results in results.items():\n",
    "    results_summary[config_name] = {\n",
    "        'mean_reward': float(config_results['mean_reward']),\n",
    "        'std_reward': float(config_results['std_reward']),\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': config_results['config']['learning_rate'],\n",
    "            'hidden_dim': config_results['config']['hidden_dim']\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"✓ Results saved to: {results_json_path}\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY - All 12 REINFORCE Configurations\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Config': name,\n",
    "        'Mean Reward': f\"{results[name]['mean_reward']:.2f}\",\n",
    "        'Std Reward': f\"{results[name]['std_reward']:.2f}\",\n",
    "        'LR': results[name]['config']['learning_rate'],\n",
    "        'Hidden': results[name]['config']['hidden_dim']\n",
    "    }\n",
    "    for name in results.keys()\n",
    "])\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4e24c",
   "metadata": {},
   "source": [
    "## Section 8: Test Best Model on Sample Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model on 5 sample episodes\n",
    "print(f\"\\nTesting best model ({best_config}) on 5 sample episodes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "env = DaladalaEnv()\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            probs = best_agent.policy(obs_tensor)\n",
    "            action = probs.argmax(dim=-1).item()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {ep+1}: Reward = {total_reward:.2f} (Steps: {steps})\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample Episodes Mean Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Sample Episodes Std Reward:  {np.std(episode_rewards):.2f}\")\n",
    "print(\"\\n✓ REINFORCE training and evaluation complete!\")\n",
    "print(f\"✓ Models and results saved to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
